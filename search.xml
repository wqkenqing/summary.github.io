<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[测试文档]]></title>
    <url>%2F2020%2F04%2F27%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2F%E6%B5%8B%E8%AF%95%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[此处简介]]></content>
  </entry>
  <entry>
    <title><![CDATA[es常用命令]]></title>
    <url>%2F2020%2F02%2F25%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fes%2Fes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[此处简介 1GET /_cluster/healt 查看节点列表 123GET /_cat/nodes?v加v将表头显示出来 索引查询所有索引1GET /_cat/indices?v 查看某个索引的映射1GET /indeName/mapping 查看某个索引的设置1GET /indeName/mapping 添加一个索引12345678910111213141516171819202122232425262728PUT /indexName&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 3, &quot;number_of_replicas&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;man&quot;: &#123; &quot;dynamic&quot;: &quot;strict&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;birthday&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;address&quot;:&#123; &quot;dynamic&quot;: &quot;true&quot;, &quot;type&quot;: &quot;object&quot; &#125; &#125; &#125; &#125; &#125; dynamic关键词说明:“dynamic”:”strict” 表示如果遇到陌生field会报错“dynamic”: true 表示如果遇到陌生字段，就进行dynamic mapping“dynamic”: “false” 表示如果遇到陌生字段，就忽略 删除索引删除单个索引 12DELETE /indexName 删除多个 12DELETE /indexName1,indexName2 添加字段映射12345678PUT /indexName/_mapping/Field&#123; &quot;properties&quot;:&#123; &quot;Field&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125; &#125;&#125; 索引的别名创建索引别名1PUT /indeName/_alias/aliasName 获取索引别名 1GET /indexName/_alias/* 查询别名对应的索引 1GET /*/_alias/aliasName 文档向索引中添加文档 自定义ID 1234567891011PUT /indexName/type/id&#123; &quot;Field1&quot;:&quot;message&quot;, &quot;Field2&quot;:&quot;message&quot;, &quot;Field3&quot;:&quot;message&quot;, &quot;Field4&quot;:&quot;message&quot;&#125;``` 2. 随机生成id POST /indexName/type{ “Field1”:”message”, “Field2”:”message”, “Field3”:”message”, “Field4”:”message”}123456后者则会自动生成id字符串3. 修改文档全文修改,即所有字段信息都要修改 PUT /indexName/type/id{ “Field1”:”update message”, “Field2”:”update message”, “Field3”:”message”, “Field4”:”message”}1部份修改 POST /indexName/type/id/_update{ “doc”: { “FIELD”: “message” }} 1脚本(再深入) POST /indexName/type/_id/_update{ “script”: { “lang”: “painless”, “source”: “ctx._source.age += 10” }}1在修改document的时候，如果该文档不存在，则使用upsert操作进行初始化 POST people/man/1/_update{ “script”: “ctx._source.age += 10”, “upsert”: { “age”: 20 }}12345### 删除文档删除单个文档 DELETE /indexName/type/id 12删除type下所有的文档 POST /indexName/type/_delete_by_query?conflicts=proceed{ “query”:{ “match_all”:{ } }} 1234### 查询文档查询单个文档 GET /indexName/type/id 12批量查询文档(待验证) GET /_mget{ “docs”: [ { “_index”: “index1”, “_type”: “type”, “_id”: 1 }, { “_index”: “index2”, “_type”: “type”, “_id”: 2 } ]} 12345678910111213```GET /indexName/type/_mget&#123;&quot;docs&quot;:[&#123; &quot;FEILD&quot;:&quot;value&quot;&#125;,&#123; &quot;FEILD2&quot;:&quot;value&quot;&#125; ]&#125; 查询所有文档 简单查询 1GET /indexName/_serach 法二 1234567POST /people/_serach&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123; &#125; &#125;&#125; 查询某些字段内容12345678910后面跟了 ?_source=field1,field2POST /people/_serach?_source=field1,field2&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123; &#125; &#125;&#125; 查询多个索引下的多个type 1GET /index1,index2/type1,type2/_search 查询所有索引下的部分type1GET /_all/type1,type2/_search 模糊查询123456789101112131415POST /indexName/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;field&quot;:&quot;message&quot; &#125; &#125; , &quot;sort&quot;:[ &#123; &quot;filed&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125; &#125; ]&#125; 注意message将会被拆分进行匹配,若message是中文,则会按切分后的每个字来匹配,若message是英语,则会是按每个单词来匹配 全文搜索(按准度) 1234567891011GET indexName/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;Field&quot;:&#123; &quot;query&quot;:&quot;val1 val2&quot;, &quot;operator&quot;:&quot;and&quot; &#125; &#125; &#125;&#125; 即Fileld 中必须有val1,val2 按匹配度查询123456789101112GET /indexName/_search&#123;&quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;Field&quot;:&#123; &quot;query&quot;:&quot;val1 val2 val3&quot; &quot;minimum_should_match&quot;:&quot;val&quot; eg:20% &#125; &#125;&#125;&#125; 即indexName,按Field中 val1 val2 val3 匹配度达到val即返回查询 高级查询简单精准查询 12345678GET /indexName/_search&#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;Field&quot;:&quot;val&quot; &#125; &#125;&#125; 即查询要完全匹配val,但若val只有一个中文,则会Field只要含有val,就会被查出 slop结合1234567891011GET /people/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;name&quot;: &#123; &quot;query&quot;: &quot;张三&quot;, &quot;slop&quot;: 3 &#125; &#125; &#125;&#125; 解读：slop是移动次数，上面案例表示“张”、“三”两个字可以经过最多挪动3次查询到！ rescore (重打分） 123456789101112131415161718192021GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125;, &quot;rescore&quot;:&#123; &quot;window_size&quot;: 50, &quot;query&quot;: &#123; &quot;rescore_query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;slop&quot;: 50 &#125; &#125; &#125; &#125; &#125;&#125; 多字段匹配查询 123456789GET /indexName/_search&#123; &quot;query&quot;:&#123; &quot;multi_match&quot;:&#123; &quot;query&quot;:&quot;val&quot; &quot;fields&quot;:[&quot;val1&quot;,&quot;val2&quot;] &#125; &#125;&#125; 在多个字段中,也是模糊查询val123456789GET /people/_search&#123; &quot;query&quot;: &#123; &quot;query_string&quot;: &#123; &quot;query&quot;: &quot;(叶良辰 AND 火) OR (赵日天 AND 风)&quot;, &quot;fields&quot;: [&quot;name&quot;,&quot;desc&quot;] &#125; &#125;&#125; 字段查询精准查询 12345678GET /indexName/_search&#123;&quot;query&quot;:&#123; &quot;term&quot;:&#123; &quot;field&quot;:&quot;val&quot; &#125;&#125;&#125; 分页查询 12345678GET /indexName/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;from&quot;:num, &quot;size&quot;:num&#125; 范围查询数据值型 123456789101112GET /people/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gt&quot;: 16, &quot;lte&quot;: 30 &#125; &#125; &#125;&#125; 日期类型 1234567891011GET /people/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;birthday&quot;: &#123; &quot;gte&quot;: &quot;2013-01-01&quot;, &quot;lte&quot;: &quot;now&quot; &#125; &#125; &#125;&#125; 1234567891011121314151617GET book/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gt&quot;: &quot;now-1M&quot; &#125; &#125; &#125;, &quot;boost&quot;: 1.2 &#125; &#125;&#125; “gt”: “now-1M”表示从今天开始，往前推一个月！ 过滤查询法一 123456789101112131415161718POST /people/man/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 20, &quot;lte&quot;: 30 &#125; &#125; &#125;, &quot;boost&quot;: 1.2 &#125; &#125;&#125; 法二 123456789101112POST /people/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;age&quot;: 18 &#125; &#125; &#125; &#125;&#125; 布尔查询should查询注意：should相当于 或 ，里面的match也是模糊匹配 12345678910111213141516171819POST /people/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;叶良辰&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;赵日天&quot; &#125; &#125; ] &#125; &#125;&#125; must查询注意：两个条件都要满足，并且这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询；“赵日天”拆分成“赵”、“日”、和“天”！ 1234567891011121314151617181920POST /people/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;叶良辰&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;赵日天&quot; &#125; &#125; ] &#125; &#125;&#125; must与filter相结合这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询 123456789101112131415161718192021222324252627POST /people/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;叶良辰&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;赵日天&quot; &#125; &#125; ], &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;age&quot;: 18 &#125; &#125; ] &#125; &#125;&#125; must_not注意：下面语句是精准匹配 123456789101112POST /people/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: &#123; &quot;term&quot;: &#123; &quot;name&quot;: &quot;叶良辰&quot; &#125; &#125; &#125; &#125;&#125; 聚合查询根据字段类型查询 123456789101112GET /people/man/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_age&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;&#125; 查询总体值 12345678910POST /people/_search&#123; &quot;aggs&quot;: &#123; &quot;grads_age&quot;: &#123; &quot;stats&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;&#125; 查询最小值 12345678910POST /people/_search&#123; &quot;aggs&quot;: &#123; &quot;grads_age&quot;: &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;&#125; 根据国家分组，然后计算年龄平均值： 12345678910111213141516171819GET /people/man/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_age&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;country&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 解决：上面的reason里面说的很清楚，将fielddata设置为true就行了： 12345678910POST /people/_mapping/man&#123; &quot;properties&quot;: &#123; &quot;country&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 排序查询123456789101112131415161718192021222324252627282930313233343536373839排序查询通常没有排到我们想要的结果，因为字段分词后，有很多单词，再排序跟我们想要的结果又出入解决办法：把需要排序的字段建立两次索引，一个排序，另一个不排序。如下面的案例：把title.raw的fielddata设置为true，是排序的；而title的fielddata默认是false，可以用来搜索index: true 是在title.raw建立索引可以被搜索到，fielddata: true是让其可以排序PUT /blog&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;auther&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: true, &quot;fielddata&quot;: true &#125; &#125; &#125;, &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;publishdate&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125;&#125; 12345678910111213GET /blog/article/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;title.raw&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; scroll查询12345当搜索量比较大的时候，我们在短时间内不可能一次性搜索完然后展示出来这个时候，可以使用scroll进行搜索比如下面的案例，可以先搜索3条数据，然后结果中会有一个_scroll_id，下次搜索就可以直接用这个_scroll_id进行搜索了 12345678GET test_index/test_type/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: &quot;_doc&quot;, &quot;size&quot;: 3&#125; step3 把scroll_id粘贴到下面的命令中再次搜索 12345GET _search/scroll&#123; &quot;scroll&quot;: &quot;1m&quot;, &quot;scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAA6FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPhZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1nAAAAAAAAADsWdk9KX2xtVHhRVUNTU0tvbDFVcmVtZwAAAAAAAAA8FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPRZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1n&quot;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[kafka]]></title>
    <url>%2F2019%2F08%2F06%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fkafka%2F</url>
    <content type="text"><![CDATA[kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic sparkstreamingkafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic flumetest kafka-console-producer.sh –broker-list localhost:9092 –topic flumetest :创建生产者 kafka-console-consumer.sh –bootstrap-server namenode:9092 –topic flume-ng Kafka相关小结kafka 相关指令kafka-server-start.sh config/server.properties &amp; 启动kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_name :创建topickafka-console-producer.sh –broker-list localhost:9092 –topic topic_name :创建生产者 kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic topic_name :创建消费者 kafka-console-producer.sh –broker-list namenode:9092 –topic sparkstreaming 删除group kafka-consumer-groups –bootstrap-server 192.168.10.100:9092,192.168.10.101:9092,192.168.10.102:9092 —group traffic_history —delete kafka java apikafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本 为例,我java项目对应的版本则是1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt; &lt;version&gt;0.8.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.8.2.1&lt;/version&gt; &lt;/dependency&gt; 以上版本搭配经由我亲测通过]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink学习]]></title>
    <url>%2F2019%2F07%2F31%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fflink%2FFlink%2F</url>
    <content type="text"><![CDATA[flink内容记录 搭建创建maven项目123456789mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-java \ -DarchetypeVersion=1.6.1 \ -DgroupId=my-flink-project \ -DartifactId=my-flink-project \ -Dversion=0.1 \ -Dpackage=myflink \ -DinteractiveMode=false 12mvn clean package -Dmaven.test.skip=true 1flink run -c myflink.demo.SocketTextStreamWordCount my-flink-project-0.1.jar 127.0.0.1 9000 DataStream APIflink程序工作解剖图 执行环境flink支持 获取已经存在的flink环境 创建一个本地环境 创建一个远程环境 DataSource预置sourceSocket-based socketTextStream(); File-based Transfomations map flatMap filter keyBy reduce fold 合计 min max sum 窗口]]></content>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs命令]]></title>
    <url>%2F2019%2F07%2F17%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fhadoop%2Fhdfs%2Fhdfs%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[hdfs常用命令 count 1该命令选项显示指定路径下的文件夹数量、文件数量、文件总大小信息，如图4-6所示。 du 1统计目录下各文件大小 touchz1创建空白文件 -stat 1“%b %n %o %r %Y”依次表示文件大小、文件名称、块大小、副本数、访问时间。]]></content>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog22%2F</url>
    <content type="text"><![CDATA[#测试环境搭建小结因一些原因，最近协助搭建测试服务器，主要涉及到了一服务器系统安装，环境配置，参数调优，软件使用，自动化建设等内容，因为主要是协助，所以着重小结我参与的部份 参数调优这里的参数调优主要针对的是服务器的调优，主要是针对出现问题后的调优，这次软件使用上大致问题有 oom ioStream 端口数不足 pid分配不够等问题 om对oom的调优主要的动作有调大分配给jvm的内存，但光调大内存不一定能解决问题，当遇到大量创建线程，但linux服务器允许该用户的执行的线程数不够时，会报无法创建的问题，严重会致使无法执行。所以还需要调大用户的进程数，查看用户信息通过ulimit -a ioStream这个是io问题，经分析出现这个问题的主要原因应该该用户下限制了最大文件打开数，所以通过ulimit -n numbers即可调大该值但有时退出后可能会重新复原，所以需要长久变更可设置在.bashrc文件中 端口数不足这是并发测试web接口时，发送的请求动作的完成需要时间，本机设置的端口区间可能无法满足大批量的端口需求，所以需要进行重新设置，主要方式是调大区间，调低端口被释放时间等 第一步，修改/etc/sysctl.conf文件，在文件中添加如下行： net.ipv4.ip_local_port_range = 1024 65000 这表明将系统对本地端口范围限制设置为1024~65000之间。请注意，本地端口范围的最小值必须大于或等于1024；而端口范围的最大值则应小于或等于65535。修改完后保存此文件。解决方案][1]解决方案2 pid分配不够等问题pid分配不够]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog21%2F</url>
    <content type="text"><![CDATA[the record for python Python 知识点标识符号在 Python 里，标识符有字母、数字、下划线组成。在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。 以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入； 以双下划线开头的 foo 代表类的私有成员；以双下划线开头和结尾的 foo__ 代表 Python 里特殊方法专用的标识，如 init() 代表类的构造函数。 Python 可以同一行显示多条语句，方法是用分号 ; 分开， 行和缩进python 最具特色的就是用缩进来写模块。缩进的空白数量是可变的，但是所有代码块语句必须包含相同的缩进空白数量，这个必须严格执行因此，在 Python 的代码块中必须使用相同数目的行首缩进空格数。建议你在每个缩进层次使用 单个制表符 或 两个空格 或 四个空格 , 切记不能混用 多行语句Python语句中一般以新行作为为语句的结束符。但是我们可以使用斜杠（ \）将一行的语句分为多行显示， Python 引号Python 可以使用引号( ‘ )、双引号( “ )、三引号( ‘’’ 或 “”” ) 来表示字符串，引号的开始与结束必须的相同类型的。其中三引号可以由多行组成，编写多行文本的快捷语法，常用语文档字符串，在文件的特定地点，被当做注释。 Python注释python 中多行注释使用三个单引号(‘’’)或三个双引号(“””)。 Python空行函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。记住：空行也是程序代码的一部分。 Python 变量类型变量存储在内存中的值。这就意味着在创建变量时会在内存中开辟一个空间。基于变量的数据类型，解释器会分配指定内存，并决定什么数据可以被存储在内存中。因此，变量可以指定不同的数据类型，这些变量可以存储整数，小数或字符。 变量赋值 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。等号（=）用来给变量赋值。 python 标准数据类型python 定义了一些标准类型，用于存储各种类型的数据。Python有五个标准的数据类型：Numbers（数字）String（字符串）List（列表）Tuple（元组）Dictionary（字典） Python数字数字数据类型用于存储数值。他们是不可改变的数据类型，这意味着改变数字数据类型会分配一个新的对象。当你指定一个值时，Number对象就会被创建：您也可以使用del语句删除一些对象的引用。del语句的语法是： del var1[,var2[,var3[….,varN]]]] 您可以通过使用del语句删除单个或多个对象的引用。例如： del vardel var_a, var_b Python支持四种不同的数字类型： int（有符号整型） long（长整型[也可以代表八进制和十六进制]） float（浮点型） complex（复数） Python字符串字符串或串(String)是由数字、字母、下划线组成的一串字符。 一般记为 :s=”a1a2···an”(n&gt;=0)它是编程语言中表示文本的数据类型。 python的字串列表有2种取值顺序: 从左到右索引默认0开始的，最大范围是字符串长度少1 从右到左索引默认-1开始的，最大范围是字符串开头 当使用以冒号分隔的字符串，python返回一个新的对象，结果包含了以这对偏移标识的连续的内容，左边的开始是包含了下边界。上面的结果包含了s[1]的值l，而取到的最大范围不包括上边界，就是s[5]的值p。 加号（+）是字符串连接运算符，星号（*）是重复操作 Python列表List（列表） 是 Python 中使用最频繁的数据类型。 python元组元组是另一个数据类型，类似于List（列表）。 元组用”()”标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表 Python 字典]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2FThread%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[sparkstreaming 窗口操作]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fspark%2Fstream2%2F</url>
    <content type="text"><![CDATA[sparkstreaming时间窗口设置 说明通过sparkstreaming设置窗口函数,可达到如,每10秒计算前30秒内数据的效果 如上 主要有两个参数 窗口大小 滑动距离 val windowedWordCounts = pairs.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10)) 如上 常用api Transformation Meaning window(windowLength, slideInterval) Return a new DStream which is computed based on windowed batches of the source DStream. countByWindow(windowLength,slideInterval) Return a sliding window count of elements in the stream. reduceByWindow(func, windowLength,slideInterval) reduceByKeyAndWindow(func,windowLength, slideInterval, [numTasks]) reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks]) countByValueAndWindow(windowLength,slideInterval, [numTasks])]]></content>
      <tags>
        <tag>sparkstreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[宽窄依赖]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fspark%2F%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[spark依赖说明 种类spark的依赖关系大致有两类 narrow dependency wide dependency 说明narrow dependency父Partition ===&gt; 子partition 多对一或一对一 flatMap ,mapToPair ,map ,filter等算子父partition ===&gt; 子partition 一对多 reduce ,group by 等. stage当一个dag串联遇到宽依赖时形成stage.一个stage对应一个task.这个task的并行度由最后一个依赖决定.应该就是说由wide dependency 的具体并行度决度.如reduce ,partition=3.就3的并行度.这里的参数可以设置. wide dependency 必定对应的有shuffle.但shuffle不一定是wide dependency 如sort orderby join 即可能发生shuffle也可能不,具体看情况. pipeline一个stage划分好后.一条数据的具体运算逻辑是会一直走完所有计算逻辑后才会落地.这是与mapreduce的区别mapreduce是计算逻辑走完落地,再启动,计算又落地. 所以说spark的效率比mapreduce高也是有这个原因.dag串联后,运算优先.]]></content>
      <tags>
        <tag>spark dependency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[List学习]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog4%2F</url>
    <content type="text"><![CDATA[此处简介 List学习 ArrayList不是线程安全的，只能用在单线程环境下，多线程环境下可以考虑用Collections.synchronizedList(List l)函数返回一个线程安全的ArrayList类，也可以使用concurrent并发包下的CopyOnWriteArrayList类 ArrayList实现了Serializable接口，因此它支持序列化，能够通过序列化传输，实现了RandomAccess接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了Cloneable接口，能被克隆。 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。 Object oldData[] = elementData;//为什么要用到oldData[]乍一看来后面并没有用到关于oldData， 这句话显得多此一举！但是这是一个牵涉到内存管理的类， 所以要了解内部的问题。 而且为什么这一句还在if的内部，这跟elementData = Arrays.copyOf(elementData, newCapacity); 这句是有关系的，下面这句Arrays.copyOf的实现时新创建了newCapacity大小的内存，然后把老的elementData放入。好像也没有用到oldData，有什么问题呢。问题就在于旧的内存的引用是elementData， elementData指向了新的内存块，如果有一个局部变量oldData变量引用旧的内存块的话，在copy的过程中就会比较安全，因为这样证明这块老的内存依然有引用，分配内存的时候就不会被侵占掉，然后copy完成后这个局部变量的生命期也过去了，然后释放才是安全的。不然在copy的的时候万一新的内存或其他线程的分配内存侵占了这块老的内存，而copy还没有结束，这将是个严重的事情。 关于ArrayList和Vector区别如下： ArrayList在内存不够时默认是扩展50% + 1个，Vector是默认扩展1倍。 Vector提供indexOf(obj, start)接口，ArrayList没有。 Vector属于线程安全级别的，但是大多数情况下不使用Vector，因为线程安全需要更大的系统开销。 ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下： 关于ArrayList的源码，给出几点比较重要的总结： 1、注意其三个不同的构造方法。无参构造方法构造的ArrayList的容量默认为10，带有Collection参数的构造方法，将Collection转化为数组赋给ArrayList的实现数组elementData。 2、注意扩充容量的方法ensureCapacity。ArrayList在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就设置新的容量为旧的容量的1.5倍加1，如果设置后的新容量还不够，则直接新容量设置为传入的参数（也就是所需的容量），而后用Arrays.copyof()方法将元素拷贝到新的数组（详见下面的第3点）。从中可以看出，当容量不够时，每次增加元素，都要将原来的元素拷贝到一个新的数组中，非常之耗时，也因此建议在事先能确定元素数量的情况下，才使用ArrayList，否则建议使用LinkedList。 3、ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法。我们有必要对这两个方法的实现做下深入的了解。 http://www.cnblogs.com/ITtangtang/p/3948555.html . LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，接口中没有定义的方法get，remove，insertList，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。LinkedList没有同步方法。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建 List时构造一个同步的List： List list = Collections.synchronizedList(new LinkedList(…)); 查看Java源代码，发现当数组的大小不够的时候，需要重新建立数组，然后将元素拷贝到新的数组内，ArrayList和Vector的扩展数组的大小不同。 http://www.tuicool.com/articles/iQZBFb http://www.cnblogs.com/azai/archive/2010/12/09/1901272.html 简化 Collections.synchronizedList(List l) Serializable 、RandomAccess 、Cloneable ensureCapacity 从上面的源码剖析可以看出这三种List实现的一些典型适用场景，如果经常对数组做随机插入操作，特别是插入的比较靠前，那么LinkedList的性能优势就非常明显，而如果都只是末尾插入，则ArrayList更占据优势，如果需要线程安全，则使用Vector或者创建线程安全的ArrayList。 在使用基于数组实现的ArrayList 和Vector 时我们要指定初始容量，因为我们在源码中也看到了，在添加时首先要进行容量的判断，如果容量不够则要创建新数组，还要将原来数组中的数据复制到新数组中，这个过程会减低效率并且会浪费资源。]]></content>
  </entry>
  <entry>
    <title><![CDATA[map总结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog9%2F</url>
    <content type="text"><![CDATA[此处简介 map总结Hashtable├-HashMap└-WeakHashMap 通用Map，用于在应用程序中管理映射，通常在 java.util 程序包中实现 HashMap Hashtable Properties LinkedHashMap IdentityHashMap TreeMap WeakHashMap ConcurrentHashMap HashMap 1最常用的Map,它根据键的HashCode 值存储数据,根据键可以直接获取它的值，具有很快的访问速度。HashMap最多只允许一条记录的键为Null(多条会覆盖);允许多条记录的值为 Null。非同步的。 Hashtable 1234与 HashMap类似,不同的是:key和value的值均不允许为null;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了Hashtale在写入时会比较慢。LinkedHashMap保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的.在遍历的时候会比HashMap慢。key和value均允许为空，非同步的。 TreeMap + 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。 + 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。 + 使用entrySet遍历的速度要比keySet快很多，是keySet的1.5倍左右。]]></content>
  </entry>
  <entry>
    <title><![CDATA[java中IO小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog7%2F</url>
    <content type="text"><![CDATA[此处简介 java中IO小结FileFile类,相关api createNewFiles(); mkdir(); mkdirs() IO流按方向，分输入，输出流in、out流当中有缓冲区，即stream.read(),可以一次性读取stream.read(Byte[]byte);针对缓冲流还有两个方法mark与resetmarkSupported 判断该输入流能支持mark 和 reset 方法。 mark用于标记当前位置；在读取一定数量的数据(小于readlimit的数据)后使用reset可以回到mark标记的位置。 FileInputStream不支持mark/reset操作；BufferedInputStream支持此操作； mark(readlimit)的含义是在当前位置作一个标记，制定可以重新读取的最大字节数，也就是说你如果标记后读取的字节数大于readlimit，你就再也回不到回来的位置了。 通常InputStream的read()返回-1后，说明到达文件尾，不能再读取。除非使用了mark/reset。 流的类型 FileInputStream ObjectInputStream DataInputStream BufferedInputStream BufferedReader ByteArrayInputStream CharArrayReader Console FileReader PipedInputStream PipedReader PushbackInputStream StringReader 流的名称 说明 FileInputStream FileInputStream 从文件系统中的某个文件中获得输入字节。 ObjectInputStream ObjectInputStream 对以前使用 ObjectOutputStream 写入的基本数据和对象进行反序列化。 DataInputStream 数据输入流允许应用程序以与机器无关方式从底层输入流中读取基本 Java 数据类型。 BufferedInputStream BufferedInputStream 为另一个输入流添加一些功能，即缓冲输入以及支持 mark 和 reset 方法的能力 ByteArrayInputStream ByteArrayInputStream 包含一个内部缓冲区，该缓冲区包含从流中读取的字节。 CharArrayReader 此类实现一个可用作字符输入流的字符缓冲区 Console 此类包含多个方法，可访问与当前 Java 虚拟机关联的基于字符的控制台设备（如果有）。 FileReader 用来读取字符文件的便捷类 PushbackInputStream PushbackInputStream 为另一个输入流添加性能，即“推回 (push back)”或“取消读取 (unread)”一个字节的能力。 Java I/O默认是不缓冲流的，所谓“缓冲”就是先把从流中得到的一块字节序列暂存在一个被称为buffer的内部字节数组里，然后你可以一下子取到这一整块的字节数据，没有缓冲的流只能一个字节一个字节读，效率孰高孰低一目了然。有两个特殊的输入流实现了缓冲功能，一个是我们常用的BufferedInputStream 一些小结带array的流自带缓冲区。支持mark与reset方法其他的要套缓冲流 存在readLine()方法的流 BufferReader read(char[],0,len)的理解将len从0位开始装入char[]中，然后通过String.valueOf(char[])转换成字符串]]></content>
  </entry>
  <entry>
    <title><![CDATA[反射小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog6%2F</url>
    <content type="text"><![CDATA[此处简介 反射小结什么是反射反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。对于反射我的理解是：正java的核心思想，万物对象，那么类的类也可被描述，如类也会有名，属性，方法的特性。皆是对类的类型的一种描述。在jvm中反射机制的存在让java也拥有了动态的特性，即我们可以在运行时才确定类的类型，即相应的方法与属性 反射的功能 在运行时获取其类的相关特性，如名称，属性，方法 可以构造一个类的对象 动态代理 反射的三种形式 Class.forName(“ “);通过Class的静态方法加载相应的相对路径，获得指定的类的类型 类.class获取类的类型 实例.getClass(); 要访问私有属性，要设置setAccessibleClass cls ~~Field file =cls.getDeclaredFields();]]></content>
  </entry>
  <entry>
    <title><![CDATA[MapReduce的核心思想]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog26%2F</url>
    <content type="text"><![CDATA[此处简介 mapreduce在hadoop中更多的承担的是计算的角色 什么是MapReduce？mapreduce源于谷歌公司为研究大规模数据处理而研发出的一种并行计算模型和方法。它将并行编程中的难点和有相对门槛的方法进行高度封装，而给开发者一套接口，让开发者理专注于自己的业务逻辑，即可让自己的代码运行在分布式集群中，大大降低了开发一些并发程序的门槛。从字面上就可以知道，MapReduce分为Map(映射)、Reduce(规约) MapReduce的核心思想MapReduce主要是两种经典函数： 映射（Mapping）将一个整体按某种规则映射成N份。并对这N份进行同一种操作。 规约（Reducing）将N份文件，按某种策略进行合并。 MapReduce的角色与动作 MapReduce包含四个组成部分，分别为Client、JobTracker、TaskTracker和Task，下面我们详细介绍这四个组成部分。 Client：作业提交的发起者， 每一个 Job 都会在用户端通过 Client 类将应用程序以及配置参数 Configuration 打包成 JAR 文件存储在 HDFS，并把路径提交到 JobTracker 的 master 服务，然后由 master 创建每一个 Task（即 MapTask 和 ReduceTask） 将它们分发到各个 TaskTracker 服务中去执行。 JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业 JobTracke负责资源监控和作业调度。JobTracker 监控所有TaskTracker 与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。 TaskTracker：TaskTracker 会周期性地通过Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量。“slot”代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲slot 分配给Task 使用。slot 分为Map slot 和Reduce slot 两种，分别供Map Task 和Reduce Task 使用。TaskTracker 通过slot 数目（可配置参数）限定Task 的并发度。 Task ： Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动。HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。 Map Task 执行过程如下图 所示：由该图可知，Map Task 先将对应的split 迭代解析成一个个key/value 对，依次调用用户 自定义的map() 函数进行处理，最终将临时结果存放到本地磁盘上, 其中临时数据被分成若干个partition，每个partition 将被一个Reduce Task 处理。 Reduce Task 执行过程下图所示。该过程分为三个阶段 ![2f9ff84d09cf424cb8a1c4d24db637c0-image.png](http://img.wqkenqing.ren//file/2017/10/2f9ff84d09cf424cb8a1c4d24db637c0-image.png) 提交作业 在作业提交之前，需要对作业进行配置 程序代码，主要是自己书写的MapReduce程序 输入输出路径 其他配置，如输出压缩等 配置完成后，通过JobClient来提交 作业的初始化 客户端提交完成后，JobTracker会将作业加入队列，然后进行调度，默认的调度方法是FIFO调试方式任务的分配 TaskTracker和JobTracker之间的通信与任务的分配是通过心跳机制完成的。 TaskTracker会主动向JobTracker询问是否有作业要做，如果自己能做，那么就会申请作业任务，这个任务可以使Map，也可能是Reduce任务任务的执行 申请到任务后，TaskTracker会做如下事情： 拷贝代码到本地 拷贝任务的信息到本地 启动Jvm运行任务状态与任务的更新 任务在运行过程中，首先会将自己的状态汇报给TaskTracker，然后由TaskTracker汇总报给JobTracker 任务进度是通过计数器来实现的。 作业的完成 JobTracker 是在接受到最后一个任务运行完成后，才会将任务标志为成功 此时会做删除中间结果等善后处理工作 MapReduce任务执行流程与数据处理流程详解就任务流程而言，上文中有些或已经涉及，有些也介绍的比较详细了。但就整体而言，不是特别具体，或这个整体性，所以在此我们再集中总结一下，即使重复内容，就也当加深印象吧。 任务执行流程详解 通过jobClient提交当mapreduce的job提交至JobTracker,提交的具体信息大致会有，conf配置内容，path，相关的Map，Reduce函数等。（数据的切片会在client上完成） JobTracker中的Master服务会完成创建一个Task（MapTask与ReduceTask），并将这个任务加载至任务队列中，等待TaskTracker来获取。同进JobTracker会处于一种监听状态，监听所有TaskTracker与Job的健康情况。面对故障时，提供服务转移等。同时它还会记录任务的执行进度，资源的使用情况，提交至任务调度器，由调度器进行资源的调度。 TaskTracker会主动向jobTacker去询问是否有任务，如果有，就会申请到作业，作业可以是map,也可以是reduce任务。TaskTracker获取的不仅是任务，还有相关的处理代码也会copy一份至本地，然后TaskTracker再针对所分配的split进行处理。TaskTracker还会周期性地通过HearBeat将本节点上的资源的使用情况和任务的进行进度汇报给JobTracker，同时接收JobTracker发过来的相关命令并执行， 当TaskTracker中的任务完成后会上报给JobTracker,而当最后一个TaskTracker完成后，JobTracker才会将任务标志为成功，并执行一些如删除中间结果等善后工作。 从这里我们知道了mapreduce中任务执行流程，但mapreduce作为hadoop的计算框架，它对数据的处理流程我们还没明确涉及，所以接下来我们再深入了解下具体的数据处理流程 数据处理流程详解这里的数据处理流程，主要指的是mapreduce执行后，Task中对split的任务具体处理的这一流程，其中还包括，数据的切片，mapTask完成后将中间结果上传等动作。下面我们来具体讨论当jobClient向JobTracker提交了任务后，数据处理流程也随之开始 在Client端将输入源的数据进行切片(split)，具体的切片机制参考后面 JobTracker中的MRAppMaster将每个Split信息计算出需要的MapTask的实例数量，然后向集群申请机器启动相应数量的mapTask进程 进程启动后，根据给定的数据切片范围进行数据处理，主要流程为a)通过inputFormat来获取RecordReader读取数据，并形成输入的KV对b)将输入KV对传递给用户定义的map（）方法，做逻辑处理，并map()方法的输出的kv对手机到缓存中(这里用到了缓存机制)简而言之map中输入时要做的事是1.反射构造InputFormat.2.反射构造InputSplit.3.创建RecordReader.4.反射创建MapperRunner 而map输出时相对复杂，主要涉及到的有Partitioner，shuffle，sort，combiner等概念，我们就来一一讨论。在map（）方法执行后，map阶段是会有处理的数据输出，正常来说，就是每个split对应的每一行。如上图MapRunner的next为false时，对输入数据的map完成，这时对存内中这些map的数据，会对其进行sort,如果我们事先设置的有combiner那么，还会对sort完的数据执行combiner(一个类reduce操作，不过是对本地数据的reduce，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。)，然后开始map的内容spill到磁盘中，如果频繁的spill对磁盘会带来较大的损耗和效率影响。所以引入了一个写缓冲区的概念，即每一个Map Task都拥有一个“环形缓冲区”作为Mapper输出的写缓冲区。写缓冲区大小默认为100MB（通过属性io.sort.mb调整），当写缓冲区的数据量达到一定的容量限额时（默认为80%，通过属性io.sort.spill.percent调整），后台线程开始将写缓冲区的数据溢写到本地磁盘。在数据溢写的过程中，只要写缓冲区没有被写满，Mappper依然可以将数据写出到缓冲区中；否则Mapper的执行过程将被阻塞，直到溢写结束。溢写以循环的方式进行（即写缓冲区的数据量大致限额时就会执行溢写），可以通过属性mapred.local.dir指定写出的目录。spill结束前 溢写线程将数据最终写出到本地磁盘之前，首先根据Reducer的数目对这部分数据进行分区（即每一个分区中的数据会被传送至同一个Reducer进行处理，分区数目与Reducer数据保持一致）即 partition，partition是一个类inputSplit()的操作，即根据有多少个ReduceTask生成多少个partition,并通过jobTracker指定给相应的ReduceTask。当spill完成后，本地磁盘中会有多个溢出文件存在。在MapTask结束前，这些文件会根据相应的分区进行合并，并排序，合并可能发生多次，具体由io.sort.factor控制一次最多合并多少个文件。 如果溢写文件个数超过3（通过属性min.num.spills.for.combine设置），会对合并且分区排序后的结果执行Combine过程（如果MapReduce有设置Combiner），而且combine过程在不影响最终结果的前提下可能会被执行多次；否则不会执行Combine过程（相对而言，Combine开销过大）。 注意：Map Task执行过程中，Combine可能出现在两个地方：写缓冲区溢写过程中、溢写文件合并过程中。 注意：Mapper的一条输出结果（由key、value表示）写出到写缓冲区之前，已经提前计算好相应的分区信息，即分区的过程在数据写入写缓冲区之前就已经完成，溢写过程实际是写缓冲区数据排序的过程（先按分区排序，如果分区相同时，再按键值排序）。 这里涉及到MapReduce的两个组件：Comparator、Partitioner。(由于篇幅的原因，这里暂不引入对这两个组件的源码分析，和自定义方式，后面有机会则单开文章讨论) 在将map输出结果作为reducTask中的输入时，会涉及到磁盘写入，网络传输等资源的限制，所以对出于节省资源的考虑，可以在对map的输出结果进行压缩。默认情况下，压缩是不被开启的，可以通过属性mapred.compress.map.output、mapred.map.output.compression.codec进行相应设置。 当MapTask任务结束后，被指定的分区ReduceTask会立即开始执行，即开始拷贝对应MapTask分区中的输出结果。Reduce Task的这个阶段被称为“Copy Phase”。Reduce Task拥有少量的线程用于并行地获取Map Tasks的输出结果，默认线程数为5，可以通过属性mapred.reduce.parallel.copies进行设置。同样：如果Map Task的输出结果足够小，它会被拷贝至Reduce Task的缓冲区中；否则拷贝至磁盘。当缓冲区中的数据达到一定量（由属性mapred.job.shuffle.merge.percent、mapred.inmem.merge.threshold），这些数据将被合并且溢写到磁盘。如果Combine过程被指定，它将在合并过程被执行，用来减少需要写出到磁盘的数据量。 随着拷贝文件中磁盘上的不断积累，一个后台线程会将它们合并为更大地、有序的文件，用来节省后期的合并时间。如果Map Tasks的输出结合使用了压缩机制，则在合并的过程中需要对数据进行解压处理。 当Reduce Task的所有Map Tasks输出结果均完成拷贝，Reduce Task进入“Sort Phase”（更为合适地应该被称为“Merge Phase”，排序在Map阶段已经被执行），该阶段在保持原有顺序的情况下进行合并。这种合并是以循环方式进行的，循环次数与合并因子（io.sort.factor）有关。sort phase 通常不是合并成一个文件，而是略过磁盘操作，直接将数据合并输入至Reduce方法中 这次合并的数据可以结合内存、磁盘两部分进行操作），即“Reduce Phase”。 通常这里还有一个“Group”的阶段，这个阶段决定着哪些键值对属于同一个键。如果没有特殊设置，只有在Map Task输出时那些键完全一样的数据属于同一个键，但这是可以被改变的。 描述至这里终于能引用mapreduce中相当重要的一个概念，即shuffle。这个词在这里该怎么定义，我暂未找到个一个比较满意的答案，但我比较喜欢有人把这个比作是搓完牌一桌子人，在下一局开始前的整个过程。即Shuffle操作，涉及到数据的partition、sort、[combine]、spill、[comress]、[merge]、copy、[combine]、merge、group，而这些操作不但决定着程序逻辑的正确性，也决定着MapReduce的运行效率。 shuffle完后进入了ReduceTask的reduce()方法中在Reduce Phase的过程中，它处理的是所有Map Tasks输出结果中某一个分区中的所有数据，这些数据整体表现为一个根据键有序的输入，对于每一个键都会相应地调用一次Reduce Function（同一个键对应的值可能有多个，这些值将作为Reduce Function的参数） 至此MapReduce的逻辑过程基本描述完成，虽然洋洋洒洒可能会有数千字，但本文的出发点就不是简析，而更多是自我概念原理部份的总结，所以力求整个流程完整详细。后面我配上一些网络图片，方便大家快速理解，结合文字加深印象。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hadoop总结(第一版---HDFS篇)]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog24%2F</url>
    <content type="text"><![CDATA[此处简介 Hadoop总结(第一版—HDFS篇)123## 什么是hadoop？&gt;hadoop是稳定的，高容错的，可大规模布署的分布式文件，存储，并行编程框架。```本文默认是已经有hadoop使用经验,所以暂不涉及具体的hadoop生态的各组件的部署和调优细节，后续单开文章来总结。但在具体讲解时会涉及说参数配置会对相关组件参生影响 具体而言，hadoop核心组件内容有：hdfs、mapredcue。所以接下来的总结主要针对这两在核心组件展开 HDFS篇什么是HDFS？分布式文件系统：分布式文件系统是一种允许文件通过网络在多台主机上分享的 文件的系统，可让多机器上的多用户分享文件和存储空间HDFS:（Hadoop Distribute File System）即hadoop分布式文件系统 主要用于适合运行在通用硬件上的分布式文件系统，特点是高度容错，适合布署在廉价服务器上，具有高吞吐量的数据访问等特点 1. 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。 2. 运行在廉价的机器上。 3. 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。 4. 适用于一次写入、多次查询的情况 5. 不支持并发写情况，小文件不合适。因为小文件也占用一个块，小文件越多（1000个1k文件）块越 多，NameNode压力越大。hdfs得部署在linux系统上 HDFS的具体内容文件、节点、数据块 HDFS主要是是围绕着这三个关键词设计的. 数据块 Block：在HDFS中，每个文件都是采用的分块的方式存储，每个block放在不同的dataNode节点上，每个block的标识是一个三元组(block id , numBytes,generationStamp),block id 具有唯一性，具体分配是由namenode节点设置，然后在datanode上建立对应的Block文件，同时建立对应的block meta文件(问题是block meta文件存放位，block size可以通过配置文件设置，所以修改block size会对以前持续化的数据有何影响?） Packet:是HDFS文件在DFSClient与DataNode之间通信的过程中文件的形式(一般一个Block对应多个Packet) Chunk:是通过程中具体传输的文件单位，发送过程以Packet的方式进行，但 一个packet包含多个Chunk,同时对于每个chunk进行checksum计算，生成checksum bytes。 PacketPacket的结构：数据包和heatbeat包 一个Packet数据包的组成结构主要分为 Packet Header 、PacketData Packet Header 中又分为：Packet Data部分是一个Packet的实际数据部分。主要内容有 一个4字节校验 Checksum Chunk部分，Chunk部分最大为512字节Packet创建过程：首先将字节流数据写入一个buffer缓冲区中，也就是从偏移量为25的位置（checksumStart）开 始写Packet数据Chunk的Checksum部分，从偏移量为533的位置（dataStart）开始写Packet数据的Chunk Data部分，直到一个Packet创建完成为止。 注意：当写一个文件的最后一个Block的最后一个Packet时，如果一个Packet的大小未能达到最大长度，也就是上图对应的缓冲区 中，Checksum与Chunk Data之间还保留了一段未被写过的缓冲区位置，在发送这个Packet之前，会检查Chunksum与Chunk Data之间的缓冲区是否为空白缓冲区（gap），如果有则将Chunk Data部分向前移动，使得Chunk Data 1与Chunk Checksum N相邻，然后才会被发送到DataNode节点 hdsf架构(主要组成是节点）主要的构成角色有：Client、NameNode、SecondayNameNode、DataNode Client：系统使用者，调用HDFS API操作文件；与NN交互获取文件元数据;与DN交互进行数据读写, 注意：写数据时文件切分由Client完成 Namenode：Master节点 （也称元数据节点）是系统唯一的管理者。负责元数据的管理(名称空间和数据块映射信息);配置副本策略；处理客户端请求 Datanode：数据存储节点(也称Slave节点)，存储实际的数据；执行数据块的读写；汇报存储信息给NN Secondary NameNode：备胎，namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode careful: 注意：在hadoop 2.x 版本，当启用 hdfs ha 时，将没有这一角色热冷备份说明：热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作 冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失hdfs构架原则 元数据与数据分离：文件本身的属性（即元数据）与文件所持有的数据分离 主/从架构：一个HDFS集群是由一个NameNode和一定数目的DataNode组成 一次写入多次读取：HDFS中的文件在任何时间只能有一个Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改。 移动计算比移动数据更划算：数据运算，越靠近数据，执行运算的性能就越好，由于hdfs数据分布在不同机器上，要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据 针对第四条的解释：在上文中交代到hdfs中的文件是以block的形式存放在集群中，所以一个文件可以是被切分成很多block存放在集群的机器中针对第四条，移动计算比移动数据更划算，是因为，从理论上讲，集群的计算能力是很方便扩展的，如服务器的硬件提升，或增加服务器等。但网络带宽等资源却很容易达到瓶颈或增加经济负担，所以将计算转移至每个block就近的机器进行计算，会比将所有的block合到一个机器上再进行计算要划算，所以，叫移动计算要比移动数据划算 NameNodeNameNode是整个文件系统的管理与计算节点，是HDFS中最复杂的一个实体，它与管理着HDFS文件系统中最重要的两个关系 HDFS文件系统中的文件目录树，以及文件的数据块索引，即每个文件对应的数据块列表 数据块和数据节点的对应关系，即某一块数据块保存在哪些数据节点的信息 第一个关系即目录树、元数据和数据块的索引信息持久化到物理存储中，具体的实现是保存在命名空间的镜像fsimage和编辑日志edits中，careful：在fsimage中，并没有记录每一个block对应到那几个Datanodes的对应表信息 第二个关系是在NameNode启动后，每个DataNode对本地的磁盘进行扫描，将本DataNode上保存的block信息上报至NameNode,Namenode在接收到每个Datanode的块信息汇报后，将接收到的块信息，以及其所在的Datanode信息等保存在内存中。HDFS就是通过这种块信息汇报的方式来完成 block -&gt; Datanodes list的对应表构建（careful）类似于数据库中的检查点，为了避免edits日志过大，在Hadoop1.X 中，SecondaryNameNode会按照时间阈值（比如24小时）或者edits大小阈值（比如1G），周期性的将fsimage和edits的合 并，然后将最新的fsimage推送给NameNode。而在Hadoop2.X中，这个动作是由Standby NameNode来完成.由此可看出，这两个文件一旦损坏或丢失，将导致整个HDFS文件系统不可用在hadoop1.X为了保证这两种元数据文件的高可用性，一般的做法，将dfs.namenode.name.dir设置成以逗号分隔的多个目录，这多个目录至少不要在一块磁盘上，最好放在不同的机器上，比如：挂载一个共享文件系统fsimage\edits 是序列化后的文件，想要查看或编辑里面的内容，可通过 hdfs 提供的 oiv\oev 命令，命令: hdfs oiv （offline image viewer） 用于将fsimage文件的内容转储到指定文件中以便于阅读,，如文本文件、XML文件，该命令需要以下参数：-i (必填参数) –inputFile 输入FSImage文件-o (必填参数) –outputFile 输出转换后的文件，如果存在，则会覆盖-p (可选参数） –processor 将FSImage文件转换成哪种格式： (Ls|XML|FileDistribution).默认为Ls示例：hdfs oiv -i /data1/hadoop/dfs/name/current/fsimage_0000000000019372521 -o /home/hadoop/fsimage.txt命令：hdfs oev (offline edits viewer 离线edits查看器）的缩写， 该工具只操作文件因而并不需要hadoop集群处于运行状态。示例: hdfs oev -i edits_0000000000000042778-0000000000000042779 -o edits.xml支持的输出格式有binary（hadoop使用的二进制格式）、xml（在不使用参数p时的默认输出格式）和stats（输出edits文件的统计信息）由此可以总结到：NameNode管理着DataNode，接收DataNode的注册、心跳、数据块提交等信息的上报，并且在心跳中发送数据块复制、删除、恢复等指令；同时，NameNode还为客户端对文件系统目录树的操作和对文件数据读写、对HDFS系统进行管理提供支持另 Namenode 启动后会进入一个称为安全模式的特殊状态。处于安全模式 的 Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。 块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode 上。 ##### Secondary NameNode 在HA cluster中又称为standby node主要作用： 1.如上文提到的合并fsimage和eits日志，将eits日志文件大小控制在一个限度下 大致流程如下 namenode 响应 Secondary namenode 请求，将 edit log 推送给 Secondary namenode ， 开始重新写一个新的 edit log Secondary namenode 收到来自 namenode 的 fsimage 文件和 edit log Secondary namenode 将 fsimage 加载到内存，应用 edit log ， 并生成一 个新的 fsimage 文件 Secondary namenode 将新的 fsimage 推送给 Namenode Namenode 用新的 fsimage 取代旧的 fsimage ， 在 fstime 文件中记下检查 点发生的时 HDFS写文件 1.x 默认的block大小是64M 2.X版本默认block的大小是 128M 如上图所示 + Client预先设置的block参数切分FIle CLient向NameNode发送写数据请求， NameNode，记录block信息，并返回可用的DataNode(具体的返回规则参考下文) client向DataNode发送block1；发送过程是以流式写入具体流程是 将64M的block1按64k的packet划分 然后将第一个packet发送给host2 host2接收完后，将第一个packet发送给host1，同时client想host2发送第二个packet host1接收完第一个packet后，发送给host3，同时接收host2发来的第二个packet 以此类推，如图红线实线所示，直到将block1发送完毕 host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。 client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。 发送完block1后，再向host7，host8，host4发送block2 当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode 节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的 方式从前一个 Datanode 复制到下一个 写入的过程，按hdsf默认设置，1T文件，我们需要3T的存储，3T的网络流量 在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去 挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份 hdfs读文件 客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例； DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法 存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，将数据从DataNode传输到客户端 到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流 一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取 block持续化结构DataNode节点上一个Block持久化到磁盘上的物理存储结构，如下图所示： 每个Block文件（如上图中blk_1084013198文件）都对应一个meta文件（如上图中blk_1084013198_10273532.meta文件），Block文件是一个一个Chunk的二进制数据（每个Chunk的大小是512字节），而meta文件是与每一个Chunk对应的Checksum数据，是序列化形式存储 — 至上我们大致了解了HDFS。正如上文提到的Hadoop的特点，高可能，高容错性。若光从上文提到的特性可能还不足以说明，如NameNode环节就提到了NameNode的重要作用，但若NameNode出现了故障，对整个机集会是毁灭性的打击，于是Hadoop也引入其它的一些手段来保存高可用，高容错。接下来我们就来探讨下 Hadoop HA的引入HA：High Available即高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。 在HA具体实现方法不同的情况下，HA框架的流程是一致的, 不一致的就是如何存储和管理日志。在Active NN和Standby NN之间要有个共享的存储日志的地方，Active NN把EditLog写到这个共享的存储日志的地方，Standby NN去读取日志然后执行，这样Active和Standby NN内存中的HDFS元数据保持着同步。一旦发生主从切换Standby NN可以尽快接管Active NN的工作; 默认并未启用 hdfs ha。SPOF方案回顾： Secondary NameNode：它不是HA，它只是阶段性的合并edits和fsimage，以缩短集群启动的时间。当NN失效的时候，Secondary NN并无法立刻提供服务，Secondary NN甚至无法保证数据完整性：如果NN数据丢失的话，在上一次合并后的文件系统的改动会丢失 Backup NameNode (HADOOP-4539)：它在内存中复制了NN的当前状态，算是Warm Standby，可也就仅限于此，并没有failover等。它同样是阶段性的做checkpoint，也无法保证数据完整性3. 手动把name.dir指向NFS（Network File System），这是安全的Cold Standby，可以保证元数据不丢失，但集群的恢复则完全靠手动4. Facebook AvatarNode：Facebook有强大的运维做后盾，所以Avatarnode只是Hot Standby，并没有自动切换，当主NN失效的时候，需要管理员确认，然后手动把对外提供服务的虚拟IP映射到Standby NN，这样做的好处是确保不会发生脑裂的场景。其某些设计思想和Hadoop 2.0里的HA非常相似，从时间上来看，Hadoop 2.0应该是借鉴了Facebook的做法 5. PrimaryNN 与StandbyNN之间通过NFS来共享FsEdits、FsImage文件，这样主备NN之间就拥有了一致的目录树和block信息；而block的 位置信息，可以根据DN向两个NN上报的信息过程中构建起来。这样再辅以虚IP，可以较好达到主备NN快速热切的目的。但是显然，这里的NFS又引入了新的SPOF6. 在主备NN共享元数据的过程中，也有方案通过主NN将FsEdits的内容通过与备NN建立的网络IO流，实时写入备NN，并且保证整个过程的原子性。这种方案，解决了NFS共享元数据引入的SPOF，但是主备NN之间的网络连接又会成为新的问题 hadoop2.X ha 原理: hadoop2.x之后，Clouera提出了QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下： 基本原理就是用2N+1台 JN 存储EditLog，每次写数据操作有大多数（&gt;=N+1）返回成功时即认为该次写成功，数据不会丢失了。当然这个算法所能容忍的是最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。这个原理是基于Paxos算法 在HA架构里面SecondaryNameNode这个冷备角色已经不存在了，为了保持standby NN时时的与主Active NN的元数据保持一致，他们之间交互通过一系列守护的轻量级进程JournalNode + 任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面， 当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的 QJM方式来实现HA的主要优势：1. 不需要配置额外的高共享存储，降低了复杂度和维护成本2. 消除spof3. 系统鲁棒性(Robust:健壮)的程度是可配置4. JN不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JN的数量增多而影响性能（因为NN向JN发送日志是并行的） datanode的fencing: 确保只有一个NN能命令DN。HDFS-1972中详细描述了DN如何实现fencing1. 每个NN改变状态的时候，向DN发送自己的状态和一个序列号2. DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active 3. 如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令 客户端fencing：确保只有一个NN能响应客户端请求，让访问standby nn的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试此时和时间 Hadoop提供了ZKFailoverController角色，部署在每个NameNode的节点上，作为一个deamon进程, 简称zkfc， FailoverController主要包括三个组件:1. HealthMonitor: 监控NameNode是否处于unavailable或unhealthy状态。当前通过RPC调用NN相应的方法完成2. ActiveStandbyElector: 管理和监控自己在ZK中的状态3. ZKFailoverController 它订阅HealthMonitor 和ActiveStandbyElector 的事件，并管理NameNode的状态 ZKFailoverController主要职责： 1. 健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态2. 会话管理：如 果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在 Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主 NN，同时标记状态为Active3. 当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN4. master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态 hadoop2.x Federation： 单Active NN的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NN进程使用的内存可能会达到上百G，NN成为了性能的瓶颈 常用的估算公式为1G对应1百万个块，按缺省块大小计算的话，大概是64T (这个估算比例是有比较大的富裕的，其实，即使是每个文件只有一个块，所有元数据信息也不会有1KB/block) 为了解决这个问题,Hadoop 2.x提供了HDFS Federation, 示意图如下： 多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务 每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储 DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况 如果需要在客户端方便的访问若干个NN上的资源，可以使用客户端挂载表，把不同的目录映射到不同的NN，但NN上必须存在相应的目录 设计优势： 改动最小，向前兼容；现有的NN无需任何配置改动；如果现有的客户端只连某台NN的话 分离命名空间管理和块存储管理 客户端挂载表：通过路径自动对应NN、使Federation的配置改动对应用透明 (与上面ha方案中介绍的最多2个NN冲突？) 至此hadoop中的hdfs高可用特性，高容错的实现又有了更深理解，但针对hdfs还一层设计实现机架感知 机架感知分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布 式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。 具体到hadoop集群，由于hadoop的HDFS对数据文件的分布式存放是按照分块block存储，每个block会有多个副本(默认为3)，并且为了数据的安全和高效，所以hadoop默认对3个副本的存放策略为：在本地机器的hdfs目录下存储一个block 在另外一个rack的某个datanode上存储一个block在该机器的同一个rack下的某台机器上存储最后一个block这样的策略可以保证对该block所属文件的访问能够优先在本rack下找到，如果整个rack发生了异常，也可以在另外的rack上找到该block的副本。这样足够的高效，并且同时做到了数据的容错。hadoop对机架的感知并非是自适应的，亦即，hadoop集群分辨某台slave机器是属于哪个rack并非是只能的感知的，而是需要 hadoop的管理者人为的告知hadoop哪台机器属于哪个rack，这样在hadoop的namenode启动初始化时，会将这些机器与rack的对 应信息保存在内存中，用来作为对接下来所有的HDFS的写块操作分配datanode列表时（比如3个block对应三台datanode）的选择 datanode策略，做到hadoop allocate block的策略：尽量将三个副本分布到不同的rack。具体实现本文不在深究，在此附上网上的一些解决方式机架感知实现1 机架感知实现2 至此对hdfs的理解与总结告一段落，后续有了新的理解再进行补充]]></content>
  </entry>
  <entry>
    <title><![CDATA[java中枚举的使用]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog3%2F</url>
    <content type="text"><![CDATA[此处简介 java中枚举的使用enum 的全称为 enumeration 是 JDK 1.5 中引入的新特性，存放在 java.lang 包中。 原始的接口定义常量 语法（定义） 遍历、switch 等常用操作 enum 对象的常用方法介绍 给 enum 自定义属性和方法 EnumSet，EnumMap 的应用 enum 的原理分析 总结 原始的接口定义常量 123456789public interface IConstants &#123; String MON = &quot;Mon&quot;; String TUE = &quot;Tue&quot;; String WED = &quot;Wed&quot;; String THU = &quot;Thu&quot;; String FRI = &quot;Fri&quot;; String SAT = &quot;Sat&quot;; String SUN = &quot;Sun&quot;;&#125; 语法（定义） 创建枚举类型要使用 enum 关键字，隐含了所创建的类型都是 java.lang.Enum 类的子类（java.lang.Enum 是一个抽象类）。枚举类型符合通用模式 Class Enum&lt;E extends Enum&gt;，而 E 表示枚举类型的名称。枚举类型的每一个值都将映射到 protected Enum(String name, int ordinal) 构造函数中，在这里，每个值的名称都被转换成一个字符串，并且序数设置表示了此设置被创建的顺序。 12345678910111213package com.hmw.test;/** * 枚举测试类 * @author &lt;a href=&quot;mailto:hemingwang0902@126.com&quot;&gt;何明旺&lt;/a&gt; */public enum EnumTest &#123; MON, TUE, WED, THU, FRI, SAT, SUN;&#125;这段代码实际上调用了7次 Enum(String name, int ordinal)：new Enum&lt;EnumTest&gt;(&quot;MON&quot;,0);new Enum&lt;EnumTest&gt;(&quot;TUE&quot;,1);new Enum&lt;EnumTest&gt;(&quot;WED&quot;,2); 遍历、switch 等常用操作 public class Test { public static void main(String[] args) { for (EnumTest e : EnumTest.values()) { System.out.println(e.toString()); } System.out.println(&quot;----------------我是分隔线------------------&quot;); EnumTest test = EnumTest.TUE; switch (test) { case MON: System.out.println(&quot;今天是星期一&quot;); break; case TUE: System.out.println(&quot;今天是星期二&quot;); break; // ... ... default: System.out.println(test); break; } } } enum 对象的常用方法介绍int compareTo(E o) 比较此枚举与指定对象的顺序。 Class getDeclaringClass() 返回与此枚举常量的枚举类型相对应的 Class 对象。 String name() 返回此枚举常量的名称，在其枚举声明中对其进行声明。 int ordinal() 返回枚举常量的序数（它在枚举声明中的位置，其中初始常量序数为零）。 String toString() 返回枚举常量的名称，它包含在声明中。 static &lt;T extends Enum&gt; T valueOf(Class enumType, String name) 返回带指定名称的指定枚举类型的枚举常量。 给 enum 自定义属性和方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344给 enum 对象加一下 value 的属性和 getValue() 的方法：package com.hmw.test;/** * 枚举测试类 * * @author &lt;a href=&quot;mailto:hemingwang0902@126.com&quot;&gt;何明旺&lt;/a&gt; */public enum EnumTest &#123; MON(1), TUE(2), WED(3), THU(4), FRI(5), SAT(6) &#123; @Override public boolean isRest() &#123; return true; &#125; &#125;, SUN(0) &#123; @Override public boolean isRest() &#123; return true; &#125; &#125;; private int value; private EnumTest(int value) &#123; this.value = value; &#125; public int getValue() &#123; return value; &#125; public boolean isRest() &#123; return false; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; System.out.println(&quot;EnumTest.FRI 的 value = &quot; + EnumTest.FRI.getValue()); &#125;&#125;输出结果：EnumTest.FRI 的 value = 5 EnumSet，EnumMap 的应用 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; // EnumSet的使用 EnumSet&lt;EnumTest&gt; weekSet = EnumSet.allOf(EnumTest.class); for (EnumTest day : weekSet) &#123; System.out.println(day); &#125; // EnumMap的使用 EnumMap&lt;EnumTest, String&gt; weekMap = new EnumMap(EnumTest.class); weekMap.put(EnumTest.MON, &quot;星期一&quot;); weekMap.put(EnumTest.TUE, &quot;星期二&quot;); // ... ... for (Iterator&lt;Entry&lt;EnumTest, String&gt;&gt; iter = weekMap.entrySet().iterator(); iter.hasNext();) &#123; Entry&lt;EnumTest, String&gt; entry = iter.next(); System.out.println(entry.getKey().name() + &quot;:&quot; + entry.getValue()); &#125; &#125;&#125; 原理分析 enum 的语法结构尽管和 class 的语法不一样，但是经过编译器编译之后产生的是一个class文件。该class文件经过反编译可以看到实际上是生成了一个类，该类继承了java.lang.Enum。EnumTest 经过反编译(javap com.hmw.test.EnumTest 命令)之后得到的内容如下： 123456789101112131415public class com.hmw.test.EnumTest extends java.lang.Enum&#123; public static final com.hmw.test.EnumTest MON; public static final com.hmw.test.EnumTest TUE; public static final com.hmw.test.EnumTest WED; public static final com.hmw.test.EnumTest THU; public static final com.hmw.test.EnumTest FRI; public static final com.hmw.test.EnumTest SAT; public static final com.hmw.test.EnumTest SUN; static &#123;&#125;; public int getValue(); public boolean isRest(); public static com.hmw.test.EnumTest[] values(); public static com.hmw.test.EnumTest valueOf(java.lang.String); com.hmw.test.EnumTest(java.lang.String, int, int, com.hmw.test.EnumTest);&#125; 所以，实际上 enum 就是一个 class，只不过 java 编译器帮我们做了语法的解析和编译而已。总结 可以把 enum 看成是一个普通的 class，它们都可以定义一些属性和方法，不同之处是：enum 不能使用 extends 关键字继承其他类，因为 enum 已经继承了 java.lang.Enum（java是单一继承）。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Restful架构]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog16%2F</url>
    <content type="text"><![CDATA[此处简介 Restful架构什么是RESTful架构Representational State Transfer:表现层状态转化 REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 “资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。 访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 综合上面的解释，我们总结一下什么是RESTful架构： 每一个URI代表一种资源； 客户端和服务器之间，传递这种资源的某种表现层； 客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 最常见的一种设计错误，就是URI包含动词。因为”资源”表示一种实体，所以应该是名词，URI不应该有动词，动词应该放在HTTP协议中。 举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。 如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是： POST /accounts/1/transfer/500/to/2 正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务： POST /transaction HTTP/1.1 Host: 127.0.0.1 from=1&amp;to=2&amp;amount=500.00 另一个设计误区，就是在URI中加入版本号： http://www.example.com/app/1.0/foo http://www.example.com/app/1.1/foo http://www.example.com/app/2.0/foo 因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分（参见Versioning REST Services）：Accept: vnd.example-com.foo+json; version=1.0 Accept: vnd.example-com.foo+json; version=1.1 Accept: vnd.example-com.foo+json; version=2.0]]></content>
  </entry>
  <entry>
    <title><![CDATA[java---GC机制]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog11%2F</url>
    <content type="text"><![CDATA[此处简介 java—GC机制JVM会有一个运行时数据区来管理内存 程序计数器(Program Counter Register) 虚拟机栈(VM Stack) 本地方法栈(Native Method Stack) 方法区(Method Area) 堆(Heap) What? – 哪些内存需要回收？而其中程序计数器、虚拟机栈、本地方法栈是每个线程私有的内存空间，随线程而生，随线程而亡。例如栈中每一个栈帧中分配多少内存基本上在类结构去诶是哪个下来时就已知了，因此这3个区域的内存分配和回收都是确定的，无需考虑内存回收的问题。 但方法区和堆就不同了，一个接口的多个实现类需要的内存可能不一样，我们只有在程序运行期间才会知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC主要关注的是这部分内存。 总而言之，GC主要进行回收的内存是JVM中的方法区和堆；涉及到多线程(指堆)、多个对该对象不同类型的引用(指方法区)，才会涉及GC的回收。 When? – 什么时候回收？堆123在面试中经常会碰到这样一个问题（事实上笔者也碰到过）：如何判断一个对象已经死去？很容易想到的一个答案是：对一个对象添加引用计数器。每当有地方引用它时，计数器值加1；当引用失效时，计数器值减1.而当计数器的值为0时这个对象就不会再被使用，判断为已死。是不是简单又直观。然而，很遗憾。这种做法是错误的！（面试时可千万别这样回答哦，我就是不假思索这样回答，然后就。。）为什么是错的呢？事实上，用引用计数法确实在大部分情况下是一个不错的解决方案，而在实际的应用中也有不少案例，但它却无法解决对象之间的循环引用问题。比如对象A中有一个字段指向了对象B，而对象B中也有一个字段指向了对象A，而事实上他们俩都不再使用，但计数器的值永远都不可能为0，也就不会被回收，然后就发生了内存泄露。。 在Java，C#等语言中，比较主流的判定一个对象已死的方法是：可达性分析(Reachability Analysis).所有生成的对象都是一个称为”GC Roots”的根的子树。从GC Roots开始向下搜索，搜索所经过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链可以到达时，就称这个对象是不可达的（不可引用的），也就是可以被GC回收了 1无论是引用计数器还是可达性分析，判定对象是否存活都与引用有关！那么，如何定义对象的引用呢？ 强引用(Strong Reference):Object obj = new Object();只要强引用还存在，GC永远不会回收掉被引用的对象。 软引用(Soft Reference)：描述一些还有用但非必需的对象。在系统将会发生内存溢出之前，会把这些对象列入回收范围进行二次回收（即系统将会发生内存溢出了，才会对他们进行回收。） 弱引用(Weak Reference):程度比软引用还要弱一些。这些对象只能生存到下次GC之前。当GC工作时，无论内存是否足够都会将其回收（即只要进行GC，就会对他们进行回收。） 虚引用(Phantom Reference):一个对象是否存在虚引用，完全不会对其生存时间构成影响。 方法区What部分我们已经提到，GC主要回收的是堆和方法区中的内存，而上面的How主要是针对对象的回收，他们一般位于堆内。那么，方法区中的东西该怎么回收呢？ 关于方法区中需要回收的是一些废弃的常量和无用的类 废弃的常量的回收。这里看引用计数就可以了。没有对象引用该常量就可以放心的回收了。 无用的类的回收。什么是无用的类呢？ 该类所有的实例都已经被回收。也就是Java堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。 12总而言之，对于堆中的对象，主要用可达性分析判断一个对象是否还存在引用，如果该对象没有任何引用就应该被回收。而根据我们实际对引用的不同需求，又分成了4中引用，每种引用的回收机制也是不同的。对于方法区中的常量和类，当一个常量没有任何对象引用它，它就可以被回收了。而对于类，如果可以判定它为无用类，就可以被回收了。 How? – 如何回收？标记-清除(Mark-Sweep)算法分为两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。缺点：效率问题，标记和清除两个过程的效率都不高；空间问题，会产生很多碎片。 复制算法将可用内存按容量划分为大小相等的两块，每次只用其中一块。当这一块用完了，就将还存活的对象复制到另外一块上面，然后把原始空间全部回收。高效、简单。缺点：将内存缩小为原来的一半。 标记-整理(Mark-Compat)算法标记过程与标记-清除算法过程一样，但后面不是简单的清除，而是让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。 分代收集(Generational Collection)算法 新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集； 老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。####一些收集器Serial收集器单线程收集器，表示在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。”Stop The World”.ParNew收集器实际就是Serial收集器的多线程版本。 并发(Parallel):指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态； 并行(Concurrent):指用户线程与垃圾收集线程同时执行，用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。Parallel Scavenge收集器该收集器比较关注吞吐量(Throughout)(CPU用于用户代码的时间与CPU总消耗时间的比值)，保证吞吐量在一个可控的范围内。CMS(Concurrent Mark Sweep)收集器CMS收集器是一种以获得最短停顿时间为目标的收集器。G1(Garbage First)收集器从JDK1.7 Update 14之后的HotSpot虚拟机正式提供了商用的G1收集器，与其他收集器相比，它具有如下优点：并行与并发；分代收集；空间整合；可预测的停顿等。 本部分主要分析了三种不同的垃圾回收算法：Mark-Sweep, Copy, Mark-Compact. 每种算法都有不同的优缺点，也有不同的适用范围。而JVM中对垃圾回收器并没有严格的要求，不同的收集器会结合多个算法进行垃圾回收。 #####内存分配Java技术体系中所提倡的自动内存管理最终可以归结为自动化的解决2个问题：给对象分配内存以及回收分配给对象的内存。 #####对象优先在Eden分配大多数情况下，对象在新生代Eden区分配。当Eden区没有足够的内存时，虚拟机将发起一次Minor GC。 Minor GC(新生代GC):指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC发生的非常频繁。 Full GC/Major GC(老年代GC):指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC。大对象直接进老年代大对象是指需要大量连续内存空间的Java对象（例如很长的字符串以及数组）。长期存活的对象将进入老年代JVM为每个对象定义一个对象年龄计数器。 如果对象在Eden出生并经历过第一次Minor GC后仍然存活，并且能够被Survivor容纳，则应该被移动到Survivor空间中，并且年龄对象设置为1； 对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度(默认为15岁，可通过参数-XX:MaxTenuringThreshold设置)，就会被晋升到老年代中。 要注意的是：JVM并不是永远的要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一般，年龄大于等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中要求的年龄。空间分配担保 在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，则进行Minor GC是安全的； 如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，则急促检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管它是有风险的； 如果小于或者HandePromotionFailure设置为不允许冒险，则这时要改为进行一次Full GC.]]></content>
  </entry>
  <entry>
    <title><![CDATA[爬虫之nutch]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog25%2F</url>
    <content type="text"><![CDATA[此处简介 这阵子主要研究的爬虫方向，主要以java为语言基础,nutch为自动框架，jsoup作为自主爬虫插件开发基础，进行了一些有针对性的实站，在这些过程中，也遇到了一些问题和心得，觉得有必要总结一下 爬虫之nutchnutch使用体验与感悟在这里之所以以nutch开篇，因为在接触爬虫之前就一直有听过它的大名，知道它是我们java语言栈中的爬虫利器，而且因为它，诞生了hadoop这一重器，后者在如今天大数据技术圈的名头，应该是无人不晓吧。所以，多种因素的促使下，让我入了nutch的坑。而本文虽是不仅针对nutch的总结，但主要还是以nutch为主。 经过一阵的实际体验,nutch给我的感觉，老实说确实是强,百科中说nutch的目的是“Nutch 致力于让每个人能很容易, 同时花费很少就可以配置世界一流的Web搜索引擎. 为了完成这一宏伟的目标, Nutch必须能够做到”,能有这么宏伟的目标，自然得有相应的实力来支撑。但就我体验来说，它确实是强，经过简单的配置后，几个简单的指令就能对页面进行抓取，但我觉得也有几个比较明显的让人体验不太好的地方 对动态生成的网页内容的抓取不理想，或要通过其它的插件，但插件的引入也不如想像般顺利 版本太多，且很多版本之间的差异明显。在后面会尝试说明一下。 资料太少，有很多场景国外都没有有效的解决策略（公网检索下），国内环境就更差了，为了搜集资料我多次去过国家图书馆，逛过书店等，都没太找到特别详尽且有针对性的资料，所以nutch的整个踩坑过程是很痛苦的，我后面会将我目前调使的nutch抓取服务配置和一些踩坑进行详细的总结，我觉得，就我之前对网上的搜集资料的掌握的情况来看，接下来我给出的总结，或者能带来一些更可靠的价值。综上，我对nutch的感悟是又爱又恨。爱得是它确实是能很便利的帮忙爬取一些东西，恨的是它又不完全是那么便利 nutch搭建与踩坑回顾前文中已经提到过nutch有很多版本，目前最新版本2.3.1，它的多个版本间是有很多差异的，比如1.X的版本间的持久层是没有抽象出来的，所以1.X版本的持久化形式比较单一。而2.X版本中持久层被抽象出来，通过一不同的配置即可实现多元的持久层存储。如mysql、hbase、avro等。但在2.3.X开始就不支持mysql，官网是明确公示过，或者说从2.2.X开始就不支持了，但就实际操作来看2.2.X还是能适配mysql的，而2.3.X在我的实际操作中是不行的。另在这里说一下，我的持久层主要以mysql为主，后面可能还会再试试hbase，而一开始不采用hbase不是我没尝试，而是尝试过，发现nutch目前适配的hbase版本太低，而我服务器上已经搭建的hbase集群环境的版本相对高了不少，无法适配。我想没有必要为此就针对hbase的版本进行更替，所以采用mysql作为持久层，这是坑一。在使用nutch之前，对其是一点都不了解的，或者说对爬虫需求也都是不熟悉的。所以，对其的功能是寄于厚望，一开始就配一个网站的首页url，就设置多线程，多层抓取。但经过多次实站后，发现有些网站中一部份或大部份都不能抓取。再随着深入观察发现，很多是动态生成的内容。更甚者，有些内容的加载是页面加载后，再发送ajax来加载页面的一些关键内容。而原本考虑nutch是支持插件扩展的，原想直接通过检索，求助于互联网，希望直接就能找到一款ajax扩展的插件，然而，几经周折，都未成行。这是坑二 其它还有一些小坑，具体我在下面搭建环节具体结细节处再提，虽小，但影响也挺大，而且比较坑人。以上都是主要通过人力暂未能直接解决的坑。所以先列出来，下面，我回顾下我的nutch搭建环节，其中会给出一些小坑与相应的解决方案 搭建nutch的搭建分为分布式与单机版，我目前主要涉猎的是nutch单机版。具体的版本选择，我尝试过nutch1.2与nutch1.7以上的所有版本，1.X与2.X的主要差异在于2.X将持久层给抽象出来了。总得来说nuthc2.x的实用性相对高些。而我以我目前具体使用的版本2.2.1为例，进行介绍step 1.获取nutch2.2.1 http://archive.apache.org/dist/这个url能定位到apache很多软件和历史版本。进入这里，然后找到nutch，下载相应的版本（该连接有时可能打不开，翻墙试试）。step 2. 对ivy下的ivy.xml与ivysetting.xml进行修改。这里就有之前提到的坑一了，这里可能配置对持久层的依赖了。我这以mysql配置。 修改${APACHE_NUTCH_HOME}/ivy/ivy.xml文件 将以下行的注释取消 123修改以下行。从默认的``` org=&quot;org.apache.gora&quot; name=&quot;gora-core&quot; rev=&quot;0.3&quot; conf=&quot;*-&gt;default&quot;/&gt; 改成123取消以下行的注释``` org=&quot;org.apache.gora&quot; name=&quot;gora-sql&quot; rev=&quot;0.1.1-incubating&quot; conf=&quot;*-&gt;default&quot; /&gt; 如果按默认的不做修改，将会在抓取网页时遇到以下错误。 然后配置ivysetting.xml，这个文件类似maven的.setting.xml文件，主要修改相应的软件仓库源，默认地址可能会出现下载缓慢的情况，建议换成国内源，贴一个我配的阿里的。 速度杠杠的。这时就可以进行编译了，通过终端进nutch文件地址进执行ant 然后配置持久层的配置文件，gora.properties 接下来配另mysql的映射文件gora-sql-mapping.xml，可以这么说，这个地方出现的坑是我遇到的坑最多的地方之一。因为我是事后总结，平日也有工任务，所以，在出现坑的时侯，我首要的考虑的是解决的它，所以解决这些问题后，可能能记得是大致是什么状况，但无法具体复现，我在这以总结，提练式的对这些小坑进行叙述，就不贴具体的错误日志了。 首先，文件中的默认配置id的大小是512.这个对于以unicode，准确的说以utf8为编码格式的mysql来说是过长的，会在inject就报sql初始化错误，id too long解决方式将id的长度改小，我设置的是180，nutch默认一般以target url拼接成id，所以一般来讲，180的id是妥妥够用的。 类似的问题还会出现一些如text ,content过长的问题，初次搭建我这先建议改小试试，后面还有根治的方式。 修改了gora-sql-mapping.xml文件后，执行抓取指令，以bin/nutch crawl urlfile -threads number -deepth num 这是nutch 普遍用的一个指行指令，这个指令执行时会默认将数据持久化到webpage的表中，这种方式缺点是不太灵活，复杂任务的时候不好操作。 另一种bin/crawl urlfile -crawlId name (这个name会作为对应的表名组成部份) “solrrl” “num”用于指定解析程度这种方式相对而言更灵活，后面我主要采用这种方式。这里有个小细节，bin/crawl 这个脚本是可以尝试更改的，它默认执行任务时只创建了50，而这种方式无法像上面那样指定创建线程数，所以我更改了crawl文件的初始值，将参数调值2000.所以可以根据自己需求，酌情更改。 bin/nutch parse 有些fetch 任务执行完成后，parse数据同步至数据库时可能会产生一些错误，那么可以通过这个指令尝试进行解析。示例：bin/nutch parse -crawlId name -all这里容易出现前面提到过的一个问题，就是在解析时，content 或text 中的内容格式不对，或text 内容中出现在了emoji等情况，都可能使解析任务中断。我经过一翻调整，算是有一个统一解决方案：在这里贴出来参考1、将数据库编码格式设置为utf8mb4；2、将text和content的gora-mapping.xml文件中添加jdbc-type=“text” 或jdbc-type=”blob”的设置，即指明其数据库中对应的类型，避免长度等问题3、上两者结合的一个情况，text存入文中的内容有编码格式，相对读取轻松，blob以二进制形式存放，取值需要转码，所以在不出错的情况下优先设为text，而这时，数据库设为utf8mb4时就不要在jdbc url 中设置utf8格式了，这样反而会出现问题。 至此 nutch日常的主要使用指令就这两个，还有些 如果bin/nutch fetch bin/nutch gernate这些相对出错较少，出错的影响和对策网上也相对较多，就不再缀述了。 至此爬虫服务的总结就要告一段落了，这篇文章体量相对较大，我是断断续续逐步完成的，所以，后面更多是凭回忆出的之前深刻影响且在网上少有明确解决方案的一些问题，给出我的解决方式，所以，如果这篇总结针对的算是nutch使用过程中，尝试更进一步的助力，而非特别基础的。如果看到此篇文章的你对nutch还有其它的一些问题，可以尝试留言，我们一起探讨。]]></content>
  </entry>
  <entry>
    <title><![CDATA[异常处理机制小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog10%2F</url>
    <content type="text"><![CDATA[此处简介 异常处理机制小结 在 Java 中，所有的异常都有一个共同的祖先 Throwable（可抛出）。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。 Throwable： 有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM 常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、 ArithmeticException）和 ArrayIndexOutOfBoundException。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。 通常，Java的异常(包括Exception和Error)分为可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）。 可查异常（编译器要求必须处置的异常）：正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。]]></content>
  </entry>
  <entry>
    <title><![CDATA[sed & awk小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog23%2F</url>
    <content type="text"><![CDATA[此处简介 sed &amp; awk小结一直想认真掌握sed与awk已经很久了，但一直未找个以特别详细的资料和时间来做这件事，正好这两天受到启发，转而翻墙搜索国外资源，有了很大的收获，趁次机会攻克下来 前言sed与awk总得来说是两样东西，本身无直接关联，做在日常使用时两者经常使用到，并且常常混合使用，所以此次小结放在一起，分总式结构进行小结 sedsed相较awk更偏于工具一点，全称应该是strem editor (即流式编辑器)。面向的是一行一行内容 使用形式sed [-nefr] [动作]选项与参数：-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。-e ：直接在命令列模式上进行 sed 的动作编辑；-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作；-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)-i ：直接修改读取的文件内容，而不是输出到终端。 动作说明： [n1[,n2]]functionn1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』 function：a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ awk尝试看awk已有些时日，整体效率不太好，但大体也有思路，现对awk进行一些简单总结，后面则进行实例操作awk的细节小点比较多，一次或无法完全总结全，但总体感觉下来发现熟悉大体模式，具体细节可以再通过即时检索解决 awk命令的基本格式是: awk &#39;/search_pattern/ { action_to_t[](http://)ake_on_matches; another_action; }&#39; file_to_parse 其中searach 与action都可省略其中之一，若action省略，那么action默认为print操作如何search省略，那么默认action针对的是每一行如 附几个操作实例如 这里是以空白区分了列，通过$后加不同的数字，表示不同的列，$0表示这一行，$1表示第一列，类推。 awk的内置变量 FILENAME:当前输入文件的名称 FNR:当前输入文件数 FS:当前环境中的分隔符，默认是空白 NF:输入文件的每行对应的列数 NR:当前是第几个记录 OFS:列输出时的分隔符，默认是空白 ORS:记录输出时的分隔符，默认是新起一行 RS输入记录中的分隔符，默认是newline character 正如以上内置变量的存在，所以，awk在正式使用进可能面临更多复杂情况，而之前那种简单模式可以无法应对。于是，常用的awk的使用形态扩充为 awk ‘BEGIN { action; }/search/ { action; }END { action; }’ input_file 这里引入了BEGIN与END两个部份，用于做一些初始化或善后处理。 awk的一些常见的匹配操作 awk ‘/sa/‘ file awk ‘$2 ~ /^sa/‘ file$number表示只匹配该列~ 表示“是”!~表示“不是” 至此一这就总结了一些较为基础的awk使用，就一般工作而言，已经能应对很多场景了。后续我会再进行更为详细的总结待续~~~]]></content>
  </entry>
  <entry>
    <title><![CDATA[maven小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog13%2F</url>
    <content type="text"><![CDATA[此处简介 maven小结什么是maven就是一款帮助程序员构建项目的工具,我们只需要告诉Maven需要哪些Jar 包，它会帮助我们下载所有的Jar，极大提升开发效率。 Maven规定的目录结构Maven基本命令 -v:查询Maven版本本命令用于检查maven是否安装成功。Maven安装完成之后，在命令行输入mvn -v，若出现maven信息，则说明安装成功 compile：编译 test:测试项目 package:打包 clean:删除target文件夹 install:安装 将当前项目放到Maven的本地仓库中。供其他项目使用 什么是Maven仓库？Maven仓库用来存放Maven管理的所有Jar包。分为：本地仓库 和 本地仓库。 本地仓库Maven本地的Jar包仓库。 中央仓库Maven官方提供的远程仓库。 当项目编译时，Maven首先从本地仓库中寻找项目所需的Jar包，若本地仓库没有，再到Maven的中央仓库下载所需Jar包。 什么是“坐标”？在Maven中，坐标是Jar包的唯一标识，Maven通过坐标在仓库中找到项目所需的Jar包。 如下代码中，groupId和artifactId构成了一个Jar包的坐标。 12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; groupId:所需Jar包的项目名artifactId:所需Jar包的模块名version:所需Jar包的版本号 传递依赖 与 排除依赖 传递依赖：如果我们的项目引用了一个Jar包，而该Jar包又引用了其他Jar包，那么在默认情况下项目编译时，Maven会把直接引用和简洁引用的Jar包都下载到本地。 排除依赖：如果我们只想下载直接引用的Jar包，那么需要在pom.xml中做如下配置：(将需要排除的Jar包的坐标写在中) 123456&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 依赖冲突若项目中多个Jar同时引用了相同的Jar时，会产生依赖冲突，但Maven采用了两种避免冲突的策略，因此在Maven中是不存在依赖冲突的。短路优先本项目——&gt;A.jar——&gt;B.jar——&gt;X.jar本项目——&gt;C.jar——&gt;X.jar声明优先若引用路径长度相同时，在pom.xml中谁先被声明，就使用谁。 聚合什么是聚合？将多个项目同时运行就称为聚合。如何实现聚合？只需在pom中作如下配置即可实现聚合： 12345&lt;modules&gt; &lt;module&gt;../模块1&lt;/module&gt; &lt;module&gt;../模块2&lt;/module&gt; &lt;module&gt;../模块3&lt;/module&gt; &lt;/modules&gt; 继承什么是继承？在聚合多个项目时，如果这些被聚合的项目中需要引入相同的Jar，那么可以将这些Jar写入父pom中，各个子项目继承该pom即可。如何实现继承？父pom配置：将需要继承的Jar包的坐标放入标签即可。 123456789&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 子pom配置： 12345&lt;parent&gt; &lt;groupId&gt;父pom所在项目的groupId&lt;/groupId&gt; &lt;artifactId&gt;父pom所在项目的artifactId&lt;/artifactId&gt; &lt;version&gt;父pom所在项目的版本号&lt;/version&gt;&lt;/parent&gt; Maven本地资源库 通常情况下，可改变默认的 .m2 目录下的默认本地存储库文件夹到其他更有意义的名称，例如 当你建立一个 Maven 的项目，Maven 会检查你的 pom.xml 文件，以确定哪些依赖下载。首先，Maven 将从本地资源库获得 Maven 的本地资源库依赖资源，如果没有找到，然后把它会从默认的 Maven 中央存储库 – http://repo1.maven.org/maven2/ 查找下载]]></content>
  </entry>
  <entry>
    <title><![CDATA[文本处理小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog14%2F</url>
    <content type="text"><![CDATA[此处简介 文本处理小结此处的文本处理主要针对的是日常工作中主要遇到的一些场景小结，小结主要围绕技能展开，但不局限于某一技术点， 文本处理的主要类型 排序 去重 合并 切割 取集 打乱排序 模糊匹配 替换 1总得来说，目前主要的文本处理手段有linux指令、shell脚本、java脚本、mr脚本、excel、sublime等几类，下现主要也围绕这部份展开 排序目前主要涉及到的排序手段是linux中的sort指令，所以这里对sort进行展开小结 ####sort排序sort排序主要的操作有12345678910111213141516171819202122232425#####sort 指令后面常接的Options![3baeaf03a24f4874818f343d3893d863-image.png](//img.wqkenqing.ren/file/2017/7/3baeaf03a24f4874818f343d3893d863-image.png)##### sort原理sort将文件/文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。排序有时往往伴随着去重，而sort则对应的有去重指令即sort -u file:忽略相同行(这里的-u其实以对应的是unique，而unique的主要作用还是去重，所以在去重部份再展开总结)##### sort的常用指令(-n、-r、-k、-t)-t:指定分隔符-n:指定以按数字的大小的形式进行排序-k:指定按那一列-r:-r是以相反顺序![5bf55dc045d846bcbe32e00202b80cbc-image.png](//img.wqkenqing.ren/file/2017/7/5bf55dc045d846bcbe32e00202b80cbc-image.png)```careful -k 有一些复杂用法，即 -k选项的语法格式： FStart.CStart Modifie,FEnd.CEnd Modifier ——-Start——–,——-End——– FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分，Start部分和End部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： $ sort -t ‘ ‘ -k 1.2 facebook.txt baidu 100 5000 sohu 100 4500 google 110 5000 guge 50 3000 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： $ sort -t ‘ ‘ -k 1.2,1.2 -nrk 3,3 facebook.txt baidu 100 5000 google 110 5000 sohu 100 4500 guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。 总得来说Linux的sort排序功能就能满足绝大部份应用场景 去重12345678910111213141516171819202122232425262728293031323334353637383940414243444546相较于排序而言，日常中去重的手段会更多一些#### 去重(linux方式)linux中的去重指令首先是刚才在前文本提到的sort -u:对文本进行排序，去重，并对重复的只保留一份。而在日常中，结合去重可能会产生更多的应用场景，即取交集、并集等大致来讲linux 主要的去重指令是uniq##### uniquniq的Options主要有![540273a5039941a39598a747888f4948-image.png](//img.wqkenqing.ren/file/2017/7/540273a5039941a39598a747888f4948-image.png)![10bff3119aaa49de976350596f94f4bf-image.png](//img.wqkenqing.ren/file/2017/7/10bff3119aaa49de976350596f94f4bf-image.png)uniq :默认只是将重复的保留一行而通过uniq实现交集与并集主要通过-d与-u实现uniq -d是只显示重复出现的行列uniq -u是只显示不重复的列这里要注意uniq -f -s的使用uniq -f nubmber :即指定忽略多少栏位开如计重uniq -s number:即指定忽略多少字任开始计重 uniq -f -s :同时出现时则按先按栏位移，再按字符移。通过uniq实现去重要先排序---针对时常出现的应用场景提供一个思路两个文本中有重复内容，但只想去掉前一个文件与后一个文件中的重复内容，保留前一个文件中的非重复内容可以通过sort file1 file2 | uniq -d &gt;tempsort file1 temp|uniq -u```即思路是:先将两个文件中的重复内容找出并写入临时文件，再将前一个文件与临时文件合并，排序去重，保留只出现一次的文件内容 其它去重方式要去重，通过java方式也能轻松实现即主要利用set等的非重复内容的特性。进行实现 切割即将文件进行切割，出于取样，或测试的需求考虑，可能需要从大文本中切割出一些小文件来。对文件的切割也有很多实现方式但从实现方式上更推荐linux指令式 切割linux指令式涉及到切割的linux指令主要有split split命令可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。 具体实现都是指令式，需要注意的地方较少，不记得时则翻阅相关文档 切割的其它方式主要擅长的还有java方式 打乱排序1打乱排序主要用的方式有awk与excel的方式 awk方式awk ‘BEGIN{ 100000*srand();}{ printf “%s %s\n”, rand(), $0}’ t |sort -k1n | awk ‘{gsub($1FS,””); print $0}’ excel方式即通过在文本中再另加一列，生成随机数，然后对随机数列进行排序从列达到打乱的效果 模糊匹配文本的模糊匹配有较多应该场景,可以再多总结 模糊匹配主要有在shell脚本中针对contains操作 和vim中的匹配操作]]></content>
  </entry>
  <entry>
    <title><![CDATA[java中的形参与实参的理解]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog8%2F</url>
    <content type="text"><![CDATA[此处简介 java中的形参与实参的理解值传递：方法调用时，实际参数把它的值传递给对应的形式参数，函数接收的是原始值的一个copy，此时内存中存在两个相等的基本类型，即实际参数和形式参数，后面方法中的操作都是对形参这个值的修改，不影响实际参数的值。 引用传递：也称为传地址。方法调用时，实际参数的引用(地址，而不是参数的值)被传递给方法中相对应的形式参数，函数接收的是原始值的内存地址；在方法执行中，形参和实参内容相同，指向同一块内存地址，方法执行中对引用的操作将会影响到实际对象。 *值传递：方法调用时，实际参数将它的值传递给对应的形式参数，函数接收到的是原始值的副本，此时内存中存在两个相等的基本类型，若方法中对形参执行处理操作，并不会影响实际参数的值。 *引用传递：方法调用时，实际参数的引用（是指地址，而不是参数的值）被传递给方法中相应的形式参数，函数接收到的是原始值的内存地址，在方法中，形参与实参的内容相同，方法中对形参的处理会影响实参的值。 1）形参为基本类型时，对形参的处理不会影响实参。2）形参为引用类型时，对形参的处理会影响实参。3）String,Integer,Double等immutable类型的特殊处理，可以理解为值传递，形参操作不会影响实参对象。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Vim笔记]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog15%2F</url>
    <content type="text"><![CDATA[此处简介 Vim笔记vi有三种工作模式： 命令模式 插入模式 和编辑模式。 插入模式 命令 作用a 在光标后附加文本A 在本行行末附加文本i 在光标前插入文本I 在本行开始插入文本o 在光标下插入新行O 在光标上插入新行 定位命令命令 作用h、方向左键 左移一个字符j、方向下键 下移一行k、方向上键 上移一行l、方向右键 右移一个字符$ 移至行尾0 移至行首H 移至屏幕上端M 移至屏幕中央L 移至屏幕下端:set nu 设置行号:set nonu 取消行号ggG 到第一行到最后一行nG 到第n行:n 到第n行 删除命令命令 作用x 删除光标所在处字符nx 删除光标所在处后n个字符dd 删除光标所在行，ndd删除n行dG 删除光标所在行到文件末尾的内容D 删除从光标所在处到行尾的内容:n1,n2d 删除指定范围的行 复制和剪切命令命令 作用yy、Y 复制当前行nyy、nY 复制当前行一下n行dd 剪切当前行ndd 剪切当前行以下n行p、P 粘贴在当前光标所在行下或行上 注：在vi中，剪切就是删除之后再粘贴 替换和取消命令命令 作用r 取代光标所在处字符R 从光标所在处开始替换字符，按Esc结束u 取消上一步操作 注：比如改变单个字符，先输入r，再输入需要更改的字符。比如将字符a改成b。这适合用于少量修改时使用 搜索和替换命令命令 作用/string 向前搜索指定字符串搜索时忽略大小写 :set icn 搜索指定字符串的下一个出现位置:%s/old/new/g 全文替换指定字符串:n1,n2s/old/new/g 在一定范围内替换指定字符串 注：n是从前往后，N是从后往前找set noic是设置大小写敏感:n1,n2s/old/new/c 替换时进行询问是否真的替换 ZZ与:wq的作用一样，都是保存退出对于readonly文件，如果是root或者改文件所有者，即使该文件没有写权限，使用:wq!也能保存该修改之后的文件。仅仅保存但不退出 :w另存为 :w /root/file.bak 其它命令导入文件 :r 文件名在vi中执行命令 :! 命令定义快捷键 :map 快捷键 触发命令范例： :map ^P I# 注：^p是这样输入的 ctrl+v+v –&gt; ^p :map ^B 0x连续行注释 :n1,n2s/^/#/g 注：^表示行首 :n1,n2s/^#//g :n1,n2s/^/\/\//g替换 :ab huhuimail huhuics@gmail.com 取消ab命令 :unan huhuimail:r !date 在vi中加入命令执行的结果快捷键插入邮箱 :map ^e ihuhuics@gmail.com 修改用户vim设置修改用户vim设置，比如能永久保存快捷键vi ~/.vimrc缺省这个文件是空的，然后可以写入一些快捷键]]></content>
  </entry>
  <entry>
    <title><![CDATA[JVM问题]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog19%2F</url>
    <content type="text"><![CDATA[此处简介 JVM问题1、堆内存溢出2、持久代内存溢出3、系统频繁FGC 框架使用不当4、错误使用框架提供API5、日志框架使用不当OS内存溢出6、某系统物理内存溢出数据库问题7、慢SQL问题 案例1、堆内存溢出JVM基础知识1、Jvm内存分为三个大区，young区，old区和perm区；其中young区又包含三个区：Edgn区、S0、S1区2、young区和old区属于heap区，占据堆内存；perm区称为持久代，不占据堆内存。堆内存溢出性能问题发现过程 查看服务器上报错日志，发现有如下报错信息［java.lang.OutOfMemoryError: Java heap space］；根据报错信息确定是jvm 堆内存空间不够导致，于是使用jvm命令（下图）查看，发现此时old区内存空间已经被占满了，同时使用jvisualvm监控工具也发现old区空间被占满（右图），整个heap区空间已经无法再容纳新对象进入。建议考虑大量数据一次性写入内存场景 持久代内存溢出现象 压测某系统接口，压测前1分钟左右tps 400多，之后Tps直降为零，后台报错日志：java.lang.OutOfMemoryError:PermGen space，通过jvm监控工具查看持久代（perm区）空间被占满，Old区空闲； 问题定位过程通过注释代码块定位问题，考虑到perm区溢出大部分跟类对象大量创建有关，故锁定问题在序列化框架使用可能有问题；对于比较棘手难解决的perm溢出问题，作者构建了一个perm区溢出的场景，可以采用如下定位方案1、添加jvm dump配置-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/dump.bin2、安装eclipse mat分析工具3、将dump文件导入eclipse，点击［Leak Suspects］，找到跟公司有关的代码进行分析此处不过多讲解，大家可以去网上查阅资料学习解决办法跟开发沟通后选择去掉msgpack0.6版本框架，采用java原成序列化框架，修改后系统tps稳定在400多，gc情况正常修改前后gc情况对比修复前类似问题如何避免 1、去掉项目无用jar包2、避免大量使用类对象、大量使用反射案例3、频繁FGC（1）系统某接口频繁FGC问题排查：先查JVM内存信息找可疑对象从内存对象实例信息中发现跟mysql连接有关，然后检测mysql配置信息 发现系统采用的是 spring框架的数据源，没有用连接池； 思考使用连接池有什么好处？连接复用、减少连接重复建立和销毁造成的大量资源消耗 然后换做hikaricp连接池做对比测试 &lt;bean id=”dataSource” class=”com.zaxxer.hikari.HikariDataSource”压测半小时未出现fgc，问题得到解决类似问题如何避免 1、研发规范统一DB连接池，避免研发误用2、减少大对象、临时对象使用 案例4、错误使用框架提供API现象某系统本身业务逻辑处理能力很快（研发本机自测tps可以到达2w多），但是接入到framework框架后，TPS最高只能到达300笔/S左右，而且系统负载很低 问题排查根据这种现象说明系统可能是堵在了某块方法上，根据这种情况一般采用线程dump的方式来查看系统具体哪些线程出现异常情况，通过线程dump 发现 ［TIMED_WAITING］状态的业务线程占比很高根据线程dump信息，找到公司包名开头的信息，然后从下往上查看线程dump信息，从信息中我们可以看到 framework.servlet.fServlet.doPost：框架api封装了servlet dopost方法做了某些操作framework.servlet.fServlet.execute：框架api执行serveltframework.process.fProcessor.process：框架api进行自身逻辑处理framework.filter.impl.AuthFilter.before：框架使用过滤器进行用户权限过滤 。。。。。。然后就是进行http请求操作由此我们断定，就是在框架进行权限校验这块堵住了。之后跟开发沟通这块的问题即可 分析思路压测端 ：net 服务器 jvm服务端：net 服务器 nginx tomcat jvm（应用程序）算法 db（mysql redis）]]></content>
  </entry>
  <entry>
    <title><![CDATA[git小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog12%2F</url>
    <content type="text"><![CDATA[此处简介 git小结常用指令小结 git init 初始化 git add 将工作区的变更提交至暂存区 git commit -m 将暂存区的内容提交至版本库 git log 查看记录 git reflog 操作记录 git –hard commit id 回到对应的id版本下 git status 查看状态 git log –pretty=oneline 简约查看log git diff 查看文件与版本库中的差异 git checkout – file 文件在工作区的修改全部撤销 git remote add origin git@github.com:michaelliao/learngit.git 添加远程库 git push -u origin master 推送分支 git clone git@github.com:michaelliao/gitskills.git 克隆 git checkout -b dev 创建并切换分支 git branch dev 创建分支 git checkout dev 切换分支 git merge dev 合并分支 git branch -d 删除分支 git stash 紧急切分支时，将工作区的变更内容暂存起来。 git stash list 查看stash列表 git stash apply 回复当前分支stash内容 git stash pop 删除stash内容 git branch -D 强行删除 git checkout -b dev origin/dev 拉下远程分支 git push origin branch-name 推送分支 git pull 从远程库拉取更新 git push origin branch-name 将本地库中的更新推送至远程库 git branch –set-upstream branch-name origin/branch-name 建立本地分支与远程库的联系 git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id；打标签 git tag -a -m “blablabla…”可以指定标签信息； git tag -s -m “blablabla…”可以用PGP签名标签； git push origin git push origin –tags git tag -d git push origin :refs/tags/]]></content>
  </entry>
  <entry>
    <title><![CDATA[String StringBuffer StringBuilder]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog2%2F</url>
    <content type="text"><![CDATA[此处简介 String StringBuffer StringBuilder String s =new String(“ok”)*（1）String ok1=new String(“ok”);（2）String ok2=“ok”;我相信很多人都知道这两种方式定义字符串，但他们之间的差别又有多少人清楚呢。画出这两个字符串的内存示意图： 1String ok1=new String(“ok”)。首先会在堆内存申请一块内存存储字符串ok,ok1指向其内存块对象。同时还会检查字符串常量池中是否含有ok字符串,若没有则添加ok到字符串常量池中。所以 new String()可能会创建两个对象. String ok2=“ok”。 先检查字符串常量池中是否含有ok字符串,如果有则直接指向, 没有则在字符串常量池添加ok字符串并指向它.所以这种方法最多创建一个对象，有可能不创建对象所以String ok1=new String(“ok”);//创建了两个对象String ok2=“ok”;//没有创建对象 比较类中的数值是否相等使用equals(),比较两个包装类的引用是否指向同一个对象时用== 123String ok=&quot;ok&quot;;String ok1=new String(&quot;ok&quot;);System.out.println(ok==ok1);//fasle 明显不是同一个对象，一个指向字符串常量池，一个指向new出来的堆内存块，new的字符串在编译期是无法确定的。所以输出false 123String ok=&quot;apple1&quot;;String ok1=&quot;apple&quot;+1;System.out.println(ok==ok1);//true String ok=&quot;apple1&quot;; int temp=1; String ok1=&quot;apple&quot;+temp; System.out.println(ok==ok1) Intern()方法但我们可以通过intern()方法扩展常量池。 intern()是扩充常量池的一个方法,当一个String实例str调用intern()方法时,java会检查常量池中是否有相同的字符串,如果有则返回其引用,如果没有则在常量池中增加一个str字符串并返回它的引用。 String类具有immutable(不能改变)性质,当String变量需要经常变换时,会产生很多变量值,应考虑使用StringBuffer提高效率。在开发时，注意String的创建方法 使用System.out.println(obj.hashcode())输出的时对象的哈希码， 而非内存地址。在Java中是不可能得到对象真正的内存地址的，因为Java中堆是由JVM管理的不能直接操作。 只能说此时打印出的Hash码表示了该对象在JAVA虚拟机中的内存位置， Java虚拟机会根据该hash码最终在真正的的堆空间中给该对象分配一个地址. 但是该地址 是不能通过java提供的api获取的 String变量连接新字符串会改变hashCode值，变量是在JVM中“连接——断开”； StringBuffer变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。 StringBuilder变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。 比较String、StringBuffer、StringBuilder性能 String类由于Java中的共享设计，在修改变量值时使其反复改变栈中的对于堆的引用地址，所以性能低。 StringBuilder是线性不安全的，适合于单线程操作，其性能比StringBuffer略高。 StringBuffer和StringBuilder类设计时改变其值，其堆内存的地址不变，避免了反复修改栈引用的地址，其性能高。 当String使用引号创建字符串时，会先去字符串池中找，找到了就返回，找不到就在字符串池中增加一个然后返回，这样由于共享提高了性能。 而new String()无论内容是否已经存在，都会开辟新的堆空间，栈中的堆内存也会改变。 性能简介StringBuilder&gt;StringBuffer&gt;String http://www.jb51.net/article/78057.htm StringBuffer中的setLength与delete的效率比较 前者主要是通过将底层的storage数组长度设置为0 后者则是另复制一份至另一空间，长度设为0所以后则的效率会相对慢一点]]></content>
  </entry>
  <entry>
    <title><![CDATA[kafka小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog17%2F</url>
    <content type="text"><![CDATA[此处简介 kafka小结消息系统术语kafka特性 分布式的 可分区的 可复制的 在普通的消息系统的功上，还有自己独特的设计 Kafka将消息以topic为单位进行归纳。将向Kafka topic发布消息的程序成为producers.将预订topics并消费消息的程序成为consumer.Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.producers通过网络将消息发送到Kafka集群，集群向消费者提供消息， 客户端和服务端通过TCP协议通信。Kafka提供了Java客户端，并且对多种语言都提供了支持。 Topics 和Logs 先来看一下Kafka提供的一个抽象概念:topic.一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区 一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区， 每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。 kafka常用指令收集查看topic的详细信息kafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic topic name 为topic增加副本kafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute创建topickafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic name为topic增加partitionkafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic namekafka生产者客户端命令kafka-console-producer.sh –broker-list localhost:9092 –topic namekafka消费者客户端命令kafka-console-consumer.sh -zookeeper localhost:2181 –from-beginning –topic namekafka服务启动kafka-server-start.sh -daemon ../config/server.properties删除topickafka-run-class.sh kafka.admin.DeleteTopicCommand –topic testKJ1 –zookeeper 127.0.0.1:2181kafka-topics.sh –zookeeper localhost:2181 –delete –topic testKJ1查看consumer组内消费的offsetkafka-run-class.sh kafka.tools.ConsumerOffsetChecker –zookeeper localhost:2181 –group test –topic name]]></content>
  </entry>
  <entry>
    <title><![CDATA[hbase]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog20%2F</url>
    <content type="text"><![CDATA[此处简介 hbase shell命令 描述alter 修改列族（column family）模式count 统计表中行的数量create 创建表describe 显示表相关的详细信息delete 删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）deleteall 删除指定行的所有元素值disable 使表无效drop 删除表enable 使表有效exists 测试表是否存在exit 退出hbase shellget 获取行或单元（cell）的值incr 增加指定表，行或列的值list 列出hbase中存在的所有表put 向指向的表单元添加值tools 列出hbase所支持的工具scan 通过对表的扫描来获取对用的值status 返回hbase集群的状态信息shutdown 关闭hbase集群（与exit不同）truncate 重新创建指定表version 返回hbase版本信息]]></content>
  </entry>
  <entry>
    <title><![CDATA[flume小结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog18%2F</url>
    <content type="text"><![CDATA[此处简介 flume小结此次flume环境的搭建是针对实际日志业务，整个过程还算顺利针对flume的引入更多的偏向应用层面。所以更多的要熟悉相关配置与参数的设置 flume的整体构思采用的是flume框架中的flume-ng。整体架构如下图log_product环节尚有争议，主要针对flume环节进行小结。原从效率上考虑，打算在跳板机上搭建直接接入hadoop的单flume节点，因为网络权限等问题，无法直接写入所以放弃。转而改为在hadoop环境中也引入一个flume节点(flume-server)。因client是单节点，所以没有必要引入fail-over机制。因此flume-server也是单节点。 ###写入hdfs时有三个参数要注意rollSizerollCountrollInterval这三个参数对写入单个hdfs文件时的大小，行，时间。 flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console &amp; flume-ng agent -n a1 -c ../conf -f flume-server.properties -Dflume.root.logger=DEBUG,console &amp;]]></content>
  </entry>
  <entry>
    <title><![CDATA[java八大数据类型总结]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Foldblog%2Fblog1%2F</url>
    <content type="text"><![CDATA[此处简介 java八大数据类型总结[TOC] 负数在电脑中的存储是用（该数值的绝对值的反码+1表示）最高位是符号位，1表示负数，0表示正数负数换算规则:负数的二进制=负数绝对值的二进制取反码+1。 byte类型:byte类型，使用一个字节存放一个数据，一个字节占八位，所以它取值范围是:1000 0000 ~ 0111 1111(-128-127) short类型 short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是： 1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型 char类型char在java中占据两个字节，即用16位表示一个char类型的数据。由于char是无符号的所以其表示范围是0-65536.当计算超过其表示范围时，系统会自动将结果转换为int类型。 boolean类型:boolean类型占用一个字节，八位二进制表示。boolean类型只有两个值true和flase。 float类型 其他特殊表示: 1.当指数部分和小数部分全为0时,表示0值,有+0和-0之分(符号位决定),0x00000000表示正0,0x80000000表示负0. 2.指数部分全1,小数部分全0时,表示无穷大,有正无穷和负无穷,0x7f800000表示正无穷,0xff800000表示负无穷. 3.指数部分全1,小数部分不全0时,表示NaN,分为QNaN和SNaN,Java中都是NaN. 可以看出浮点数的取值范围是:2^(-149)~~(2-2^(-23))*2^127,也就是Float.MIN_VALUE和Float.MAX_VALUE. double类型double类型占8个字节，一共是64位二进制表示。数符加尾数占48位，指数符加指数占16位.取值换算方式和float的换算方式一样。但是在使用float和double时最好先分析好目标数据的精度和性能要求，如果能够使用float满足的坚决不适用double，因为double类型使用内存占用是float的两倍，运算速度远不如float。 short类型 short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是： 1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型。short类型使用时除了要注意取值范围 int类型int类型，在Java中使用的是四个字节保存一个数据，一共是32为二进制表示，同上述的一样,取值范围:1000 0000 0000 0000 0000 0000 0000 0000 - 0111 1111 1111 1111 1111 1111 1111 1111 (-2^32~2^31) long类型long类型是Java的基础类型，使用8个字节存储一个数值，一共是64位二进制数。取值范围是（-2^64-2^63 八大常用类型的最大值与最小值float max=3.4028235E38 float min=1.4E-45 double max=1.7976931348623157E308 double max=4.9E-324 byte max=127 byte min=-128 char max=? char min= short max=32767 short min=-32768 int max=2147483647 int min=-2147483648 long max=9223372036854775807 long min=-9223372036854775808 数据类型 byte类型 short类型 char类型 boolean类型 float类型 int类型 double类型 long类型 所占字节数 2 2 2 8 4 4 8 8]]></content>
  </entry>
  <entry>
    <title><![CDATA[updateStateByKey&mapStateWithKey]]></title>
    <url>%2F2019%2F07%2F15%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fspark%2Fstream%2F</url>
    <content type="text"><![CDATA[spark中如何实现全局count 说明两种方式都可以实现对同一key的累计统计 区别updateStateByKey会返回无增量数据的状态,所以会相对较大的数据资源开销mapStateWithKey 相当于增量统计 使用updateStateByKey : 12345678910111213141516public static Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunctionByUpdate() &#123; Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunction = (values, s1) -&gt; &#123; Integer newSum = 0; if (s1.isPresent()) &#123; newSum = s1.get(); &#125; Iterator&lt;Integer&gt; i = values.iterator(); while (i.hasNext()) &#123; newSum += i.next(); &#125; return Optional.of(newSum); &#125;; return updateFunction; &#125; mapStateWithKey : 12345678910public static Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunctionByMap() &#123; Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunction2 = (word, one, state) -&gt; &#123; int sum = one.or(0) + (state.exists() ? state.get() : 0); Tuple2&lt;String, Integer&gt; output = new Tuple2&lt;String, Integer&gt;(word, sum); state.update(sum); return output; &#125;; return updateFunction2; &#125;]]></content>
      <tags>
        <tag>sparkstream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yarn配置细节]]></title>
    <url>%2F2019%2F06%2F13%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2FYarn%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[此处简介 Yarn配置细节##内存,核数设置 1234567891011121314151617181920212223242526272829303132 &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt;&lt;/property&gt;&lt;!--该配置用于配置任务请求时的资源. --&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx3276m&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[SparkSql]]></title>
    <url>%2F2019%2F06%2F13%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fspark%2Fsql%2FSparkSql%2F</url>
    <content type="text"><![CDATA[1spark sql 相关内容 sparksql]]></content>
  </entry>
  <entry>
    <title><![CDATA[flume记录]]></title>
    <url>%2F2019%2F06%2F13%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fflume%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[此处简介 flume记录from kafka 123456789101112131415161718192021222324252627282930313233343536373839a1.sources = source1a1.sources.source1.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.source1.channels = c1a1.sources.source1.batchSize = 5000a1.sources.source1.batchDurationMillis = 2000a1.sources.source1.zookeeperConnect = localhost:2181#a1.sources.source1.kafka.brokerList = localhost:9092a1.sources.source1.kafka.bootstrap.servers = localhost:9092a1.sources.source1.topic = flumetesta1.sources.source1.kafka.consumer.group.id = custom.g.ida1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 10000a1.channels.c1.byteCapacityBufferPercentage = 20a1.channels.c1.byteCapacity = 800000a1.sinks = k1a1.sinks.k1.type = file_rolla1.sinks.k1.channel = c1a1.sinks.k1.sink.directory = /home/hadoop/testfile/flume 这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz其它版本可能会有request header 问题.另外还遇到了指定topic 和 zookeeper的问题. 执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger=INFO,console flume 采集到kafka123456789101112131415161718192021222324252627agent.sources=r1agent.sinks=k1agent.channels=c1agent.sources.r1.type=execagent.sources.r1.command=tail /root/tomcat/logs/catalina.outagent.sources.r1.restart=trueagent.sources.r1.batchSize=1000agent.sources.r1.batchTimeout=3000agent.sources.r1.channels=c1agent.channels.c1.type=memoryagent.channels.c1.capacity=102400agent.channels.c1.transactionCapacity=1000agent.channels.c1.byteCapacity=134217728agent.channels.c1.byteCapacityBufferPercentage=80agent.sinks.k1.channel=c1agent.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSinkagent.sinks.k1.kafka.topic=sparkstreamingagent.sinks.k1.kafka.zookeeperConnect=47.102.199.215:2181#agent.sinks.k1.kafka.bootstrap.servers=47.102.199.215:9092agent.sinks.k1.kafka.brokerList =47.102.199.215:9092agent.sinks.k1.serializer.class=kafka.serializer.StringEncoderagent.sinks.k1.flumeBatchSize=1000agent.sinks.k1.useFlumeEventFormat=true]]></content>
  </entry>
  <entry>
    <title><![CDATA[CDH搭建细节]]></title>
    <url>%2F2019%2F05%2F27%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2FCDH%2F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[….. CDH安装准备 ubuntu ulimit]]></content>
  </entry>
  <entry>
    <title><![CDATA[hbaes操作]]></title>
    <url>%2F2019%2F05%2F17%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fhadoop%2Fhbase%2Fhbase%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[对hbase常规api进行封装 hbase日常api类封装]]></content>
      <tags>
        <tag>常规api封装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark操作.md]]></title>
    <url>%2F2019%2F05%2F17%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fspark%2Fspark%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[spark编程积累 spark编程Inputhdfs操作hdfs比较常规,直接通过context.textfile(path) //即可实现 hbasehbase 则要通过newAPIHadoopRDD来实现12JavaPairRDD&lt;ImmutableBytesWritable, Result&gt; javaRDD = jsc.newAPIHadoopRDD(HbaseOperate.getConf(), TableInputFormat.class, ImmutableBytesWritable.class, Result.class); 这里要特别说明的是,这里的conf承担了更多的责任,如指定表名,指定scan传输字符串等.12345Configuration hconf = HbaseOperate.getConf(); Scan scan = new Scan(); hconf.set(TableInputFormat.INPUT_TABLE, "company"); hconf.set(TableInputFormat.SCAN, convertScanToString(scan)); 参考以上这段代码 另1234static String convertScanToString(Scan scan) throws IOException &#123; ClientProtos.Scan proto = ProtobufUtil.toScan(scan); return Base64.encodeBytes(proto.toByteArray()); &#125; 以上是为实现scan指令传输字符的封装. 两者底层都是通过persist实现]]></content>
  </entry>
  <entry>
    <title><![CDATA[Lambda&Stream.md]]></title>
    <url>%2F2019%2F05%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2FLamda%E7%A7%AF%E7%B4%AF%2F</url>
    <content type="text"><![CDATA[Lambda&amp;Stream积累 LambdaLambda主要是一个类语法长糖,尽量为java引入函数编程等实现,细节后续再补充 Streamjava8提拱的新特性之一就有stream.stream主要是针对集合的处理类.提供了一系列集合处理方式.配合使用lambda写出简介优美的代码 Stream的使用通过如123456List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.stream();//即可以开启串行流;list.parallelStream().filter(a -&gt; &#123; return a &gt; 20; &#125;);//开启并行流 串行流即内部单线程顺序执行,并行则是启用多线程执行.后者并不一定效率就比前者高.因为并行执行启用分配线程资源时同样要消耗时间和资源,在一定量级下,前者的执行效率一度要高过后者. 我这里对三种对集合的处理形式的比较,可以简单参考一下 stream 串行流 parallelStream 并行流 常规循环式 12345678910111213141516171819202122232425262728293031List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 100000; i++) &#123; list.add(getRandomNum()); &#125; DateUtil.setBegin(); list.stream().filter(a -&gt; &#123; return a &gt; 20; &#125;); DateUtil.setStop(); System.out.println("串行耗时"+DateUtil.calCostTime()); DateUtil.setBegin(); list.parallelStream().filter(a -&gt; &#123; return a &gt; 20; &#125;); DateUtil.setStop(); System.out.println("并行耗时"+DateUtil.calCostTime()); int count = 0; DateUtil.setBegin(); for (int l : list) &#123; if (l &gt; 20) &#123; count++; &#125; &#125; DateUtil.setStop(); System.out.println("循环耗时"+DateUtil.calCostTime()); 经由相当量次的测试后,我觉得如果要对集合中的数据进行遍历操作,根据量级的不同,建议低量级还是采用普通循环,量级特别大,可考虑用并行流.书写方便,又不是大批量数据处理操作可以直接采用串行流 Stream的操作分类 Intermediate Terminal Short-circuiting]]></content>
      <tags>
        <tag>日常总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs操作细节]]></title>
    <url>%2F2019%2F05%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fhadoop%2Fhdfs%2Fhdfs%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[针对hdfs一些较细节的api封装 hdfs操作常规操作 创建文件 写数据 删除文件 上传文件 下载文件 断点续写 123错误： java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try 原因： 无法写入；我的环境中有3个datanode，备份数量设置的是3。在写操作时，它会在pipeline中写3个机器。默认replace-datanode-on-failure.policy是DEFAULT,如果系统中的datanode大于等于3，它会找另外一个datanode来拷贝。目前机器只有3台，因此只要一台datanode出问题，就一直无法写入成功。 1234567891011121314&lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.policy&lt;/name&gt; &lt;value&gt;NEVER&lt;/value&gt; &lt;/property&gt; 对于dfs.client.block.write.replace-datanode-on-failure.enable，客户端在写失败的时候，是否使用更换策略，默认是true没有问题对于，dfs.client.block.write.replace-datanode-on-failure.policy，default在3个或以上备份的时候，是会尝试更换结点尝试写入datanode。而在两个备份的时候，不更换datanode，直接开始写。对于3个datanode的集群，只要一个节点没响应写入就会出问题，所以可以关掉。]]></content>
  </entry>
  <entry>
    <title><![CDATA[hadoop高可用模式搭建]]></title>
    <url>%2F2019%2F05%2F16%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fhadoop%2FhadoopHA%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[发现对hadoop的相关版本的组件,进程还有些模糊,借着针对hadoopHA模式搭建的过程,对hadoop进行一次细统的回顾. hadoop HA搭建与总结什么是HAHA即高可用 HA相关配置core-site.xml基本一致 hdfs-site.xml这里有明显差别hadoop2.X与hadoop1.X的高可能中的明显差异就是从这里开始的.2.x 引入了nameservice. 该nameservice可支持最大两个namenode.1.x img 和edits统一放置在namenode上.2.x 则通过journalnodes来共享edits日志.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 为namenode集群定义一个services name --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- nameservice 包含哪些namenode，为各个namenode起名 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt; &lt;value&gt;namenode,datanode1&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为master188的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.namenode&lt;/name&gt; &lt;value&gt;namenode:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为master189的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.datanode1&lt;/name&gt; &lt;value&gt;datanode1:9000&lt;/value&gt; &lt;/property&gt; &lt;!--名为master188的namenode的http地址和端口号，用来和web客户端通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.namenode&lt;/name&gt; &lt;value&gt;namenode:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为master189的namenode的http地址和端口号，用来和web客户端通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.datanode1&lt;/name&gt; &lt;value&gt;datanode1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode间用于共享编辑日志的journal节点列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://namenode:8485;datanode1:8485;datanode2:8485/ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定该集群出现故障时，是否自动切换到另一台namenode --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled.ns1&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- journalnode 上用于存放edits日志的目录 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/dfs/data/dfs/journalnode&lt;/value&gt; &lt;/property&gt; &lt;!-- 客户端连接可用状态的NameNode所用的代理类 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 一旦需要NameNode切换，使用ssh方式进行操作 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- connect-timeout超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; mapreduce-site.xml变动不大 yarn-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&lt;?xml version="1.0"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;!-- 启用HA高可用性 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定resourcemanager的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定rm1的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;namenode&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定rm2的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;datanode1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定当前机器master188作为rm1 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper集群机器 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;namenode:2181,datanode1:2181,datanode2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;kuiqwang&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://namenode:19888/tmp/logs/hadoop/logs/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/logs/yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/logs/userlogs&lt;/value&gt; &lt;/property&gt;&lt;!--内存,核数大小配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx3276m&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; HA过程中主要用到的操作命令当配置文件完成后,先启动journalnode,以助namenode 和standby node 共享edits文件hadoop-daemon.sh 然后再进行namdnode格式化,hadoop namenode -format进行namenode格式化当namenode格式化完成后可以先启动该节点的namenodehadoop-daemon.sh start namenode然后再在另一namdnode节点执行hdfs namenode -bootstrapStandby到这可以将之前的journalnode停用,然后start-dfs.sh 因为要用到zookeeper协助同步配置文件与操作日志,所以这里可以先对zookeeper进行hdfs内容的格式化hdfs zkfc –formatZK然后启动FailOver进程hadoop-daemon.sh start zkfc至此则是这些进程然后启用yarn.即start-yarn.sh到这里HA过程中用到的一些常用指令大致总结完成 至此 hadoop HA的常规总结完成.后续再补充一些细节,如standy 节点切的,与切换机制.HA背后的运作机制,与效果]]></content>
  </entry>
  <entry>
    <title><![CDATA[kafka学习]]></title>
    <url>%2F2019%2F05%2F06%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fkafka%2Fkafka%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[[ x ] Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。 kafka 发送模式通过producer.type设置,可以设置producer的发送模式,具体参数据有producer.type=false即同步(默认就是同步),设置为true为异步,即以batch形式像broker发送信息.(这里的batch可以设置)还有一种oneway.即通过对ack的设置即可实现,ack=0时,即为oneway,只管发,不管是否接收成功.-1则是全部副本接收成功才算成功. kakfa消费模式 at last one at most one exactly one]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce组件总结]]></title>
    <url>%2F2019%2F04%2F19%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E5%AD%A6%E4%B9%A03%2F</url>
    <content type="text"><![CDATA[spark-core,spark-streaming再深造 spark go on初始规划 spark-corespark-streaming]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据分享]]></title>
    <url>%2F2019%2F04%2F15%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[开头语工欲善其事，必先利其器本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地. 技术栈简介 数据采集 数据存储 数据治理(清洗&amp;处理) 数据应用 产品落地 我又根据不同组件的特性将他们分 采集类 存储类 计算处理类 传输类 管理类 其它类 下面开始具体介绍 采集类数据源: 日志 业务数据 公网数据(爬虫) 文本数据 出行数据(gps,手机定位等) sqoop flume crawler datax kettle elk Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展 Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。 crawler , jsoup ,httpclient, nutch 等. elk 集中式日志系统 ELK 协议栈详解 存储类 hdfs hbase hive mongdb redis RDBMS hdfs* 分布式文件存储系统 * 提供了高可靠性、高扩展性和高吞吐率的数据存储服务 * hdfs典型结构：物理结构+逻辑结构 * 文件线性切割成Block：偏移量（offset） * Block分散存储在集群节点中 * 单一文件Block大小一致，文件与文件可以不一致 * Block可以设置副本数，副本分散在不同的节点中 * 副本数不要超过节点数量 * 文件上传可以设置Block大小和副本数 * 已上传的文件Block副本数可以调整，大小不变 * 只支持一次写入多次读取，同一时刻只有一个写入者 * 只能追加，不能修改 hbaseBase是一个构建在HDFS上的分布式列存储系统；Base是基于Google BigTable模型开发的，典型的key/value系统；Base是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储； 大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型 openTSDB基于Hbase的分布式的，可伸缩的时间序列数据库。主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。 solr &amp; Phoenix二级索引 hiveive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。底部计算引擎还可以用用Tez, spark等. ImpalaImpala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。 基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点 对内存依赖大,稳定性不如hive 相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求. 计算处理类 mapreduce mapreduce on oozie ,on tez spark flink mapreduceMapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。 分布式计算；移动计算而不移动数据。 spark相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.其中比较有代表性的主要就是spark&amp;storm.也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.这里主要介绍spark spark主要组成有以下 spark-core spark-streaming spark-sql spark-mlib spark-graphX。 spark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.spark-core的主要内容就是对RDD的操作RDD的创建 -&gt;RDD的转换 -&gt;RDD的缓存 -&gt;RDD的行动 -&gt;RDD的输出 spark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作 spark-sqlSpark SQL 提供了查询结构化数据及计算结果等信息的接口.查询结果以 Datasets and DataFrames 形式返回 … flink/blink略 传输类kafkaKafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据 管理类 Hue cloudera-managerue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌. 其它类 zookeeper ,yarn等zookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成 各类组件架构以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境. 于我个人经历与平时了解来讲,一般的架构主要如下另: 总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等. 通过对上图的架构的拆解,再组合,可能还会有以下组织架构. 数据仓库可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份. ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。 DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。 ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。 星型模型 星型模型中有两个重要的概念：事实表和维度表。事实表：一些主键ID的集合，没有存放任何实际的内容维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。 数据中心:概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如 大数据基础平台 数据仓库 DMP平台 相关应用平台如推荐系统,报表系统,可视化平台等. 数据中台:这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想”大中台,小前台”的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据. 应用落地公共服务 交通出行 智慧城市 … 产品应用 用户画像 征信模型 推荐系统 精确营销 前沿科学(无人驾驶,人工智能,AR等) 结语以上,就是我今天分享的主要内容.今天的主题是”器”,但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸. 另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行. 附上图中涉及到的技术栈]]></content>
      <tags>
        <tag>日常总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark实战]]></title>
    <url>%2F2019%2F03%2F21%2Fspark%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[spark实战spark-core]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce组件总结]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fmapreduce%E7%BB%84%E4%BB%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[mapreduce组件总结 相关组件大致有 Inputformat Inputsplit ReadRecorder mapper Combiner Partioner Reduce GroupComparator Reduce shuffle 1shuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程. shuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作) 具体流程是map out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出 spill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件 当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&amp;merge后的文件.当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种 memory to memory memory to disk disk to disk默认1是不开启的. copy phase 完成后,是reduceTask 中的 sort phase即对merge 中的文件继续进行sort and group . 当sort phase 完成.则开启reduce phase .到此shuffle正式完成. ##二次排序 mapreduce 常见的辅助排序 partitioner key的比较Comparator 分组函数Grouping Comparator joinmap join ,semi join ,reduce join ##]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[准备小结]]></title>
    <url>%2F2019%2F03%2F14%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2F%E5%87%86%E5%A4%87%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[准备小结 hdfs存储机制是怎样的?client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanodenamenode收到的client信息后，发送确信信息给datanodedatanode同时收到namenode和datanode的确认信息后，提交写操作。 hadoop中combiner的作用是什么?当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。 你们数据库怎么导入hive 的,有没有出现问题在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。 hdfs-site.xml的3个主要属性?dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)dfs.data.dir决定的是数据存储的路径fs.checkpoint.dir用于第二Namenode 下列哪项通常是集群的最主要瓶颈磁盘 IO答案：C 磁盘首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？1.cpu 处理能力强2.内存够大，所以集群的瓶颈不可能是 a 和 d3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。 关于 SecondaryNameNode 哪项是正确的？它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 mapreduce的原理?MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker是用于执行工作的。一个Hadoop集群中只有一台JobTracker。在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。 HDFS存储的机制?写流程：client链接namenode存数据namenode记录一条数据位置信息（元数据），告诉client存哪。client用hdfs的api将数据块（默认是64M）存储到datanode上。datanode将数据水平备份。并且备份完将反馈client。client通知namenode存储块完毕。namenode将元数据同步到内存中。另一块循环上面的过程。 读流程举一个简单的例子说明mapreduce是怎么来运行的 ?MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。 Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。Mapper任务的执行过程详解 每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段： 第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是 172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由 一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。 第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一 行的起始位置(单位是字节)，“值”是本行的文本内容。 第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会 调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。 第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、 山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer 任务运行的数量。默认只有一个Reducer任务。第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值 对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入 第六阶段 如果没有，直接输出到本地的Linux文件中。 第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。 归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。 Reducer任务的执行过程详解每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中。在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。 了解hashMap 和hashTable吗介绍下，他们有什么区别。为什么重写equals还要重写hashcode因为equals比较的是内容是一致.但hashcode 说一下map的分类和常见的情况 hashmap,hashtable,treemap,LinkedHashMap 根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复Hashmap是一个最常用的Map 它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的 最多只允许一条记录的键为Null;允许多条记录的值为 Null; HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。 如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMapHashtableHashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空; 它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢LinkedHashMap是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关TreeMap实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的 HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap Object若不重写hashCode()的话，hashCode()如何计算出来的？hashcode采用的是 spark1. spark的有几种部署模式，每种模式特点？本地模式本地模式分三类 local：只启动一个executor local[k]: 启动k个executor local[*]：启动跟cpu数目相同的 executor cluster模式cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源） standalone模式分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础 Spark on yarn模式分布式部署集群，资源和任务监控交给yarn管理粗粒度资源分配方式，包含cluster和client运行模式cluster 适合生产，driver运行在集群子节点，具有容错功能client 适合调试，dirver运行在客户端 2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？Spark core是其它组件的基础，spark的内核主要包含：有向循环图、RDD、Lingage、Cache、broadcast等 SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统将流式计算分解成一系列短小的批处理作业 Spark sql：能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询 MLBase是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。 GraphX是Spark中用于图和图并行计算 spark有哪些组件master：管理集群和节点，不参与计算。worker：计算节点，进程本身不参与计算，和master汇报。Driver：运行程序的main方法，创建spark context对象。spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。client：用户提交程序的入口。 https://blog.csdn.net/yirenboy/article/details/47441465]]></content>
      <tags>
        <tag>小结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive总结]]></title>
    <url>%2F2018%2F12%2F24%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fhive%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Hive相关点小结 启动指令 hive == hive –service cli不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。 启动hiveserver2hive –service hiveserver2 beeline工具测试使用jdbc方式连接beeline -u jdbc:hive2://localhost:10000 1.managed table管理表。删除表时，数据也删除了 2.external table外部表。删除表时，数据不删 建表:CREATE TABLE IF NOT EXISTS t2(id int,name string,age int)COMMENT ‘xx’ //注释ROW FORMAT DELIMITED //行分隔符FIELDS TERMINATED BY ‘,’ //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改STORED AS TEXTFILE ; 外部表: CREATE TABLE IF NOT EXISTS t2(id int,name string,age int) COMMENT ‘xx’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ STORED AS TEXTFILE ; 分区表，桶表分区表Hive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) //按照年月进行分区 ROW FORMAT DELIMITED //行分隔符 FIELDS TERMINATED BY ‘,’ ; //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改load data local inpath ‘/home/zpx/customers.txt’ into table t3 partition 分桶表这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS //创建3个通桶表，按照字段id进行分桶 ROW FORMAT DELIMITED //行分隔符 FIELDS TERMINATED BY &apos;,&apos; ; load data local inpath ‘/home/centos/customers.txt’ into table t4 ; 导入数据load data local inpath ‘/home/zpx/customers.txt’ into table t2 ; //local上传文件load data inpath ‘/user/zpx/customers.txt’ [overwrite] into table t2 //分布式文件系统上移动文件 建视图Hive也可以建立视图，是一张虚表，方便我们进行操作. create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ; Hive的严格模式Hive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。使用了严格模式之后主要对以下3种不良操作进行控制： 1.分区表必须指定分区进行查询。2.order by时必须使用limit子句。3.不允许笛卡尔积。 Hive的动态分区像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区set hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式 Hive的排序Hive也提供了一些排序的语法，包括order by,sort by。 order by=MapReduce的全排序sort by=MapReduce的部分排序distribute by=MapReduce的分区 selece …….from …… order by 字段；//按照这个字段全排序 selece …….from …… sort by 字段； //按照这个字段局部有序 selece 字段…..from …… distribute by 字段；//按照这个字段分区特别注意的是： 在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select …from ….以及使用分区表的selece ….from……where …..不会开启 distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并 select 字段a,……..from …….distribute by字段a，sort by字段如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：select 字段a,……..from …….cluster by 字段 函数 show functions; 展示相关函数 desc function split; desc function extended split; //查看函数的扩展信息 用户自定义函数（UDF）具体步骤如下： （1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。（2）.导出jar包，通过命令添加到hive的类路径。$hive&gt;add jar xxx.jar（3）.注册函数$hive&gt;CREATE TEMPORARY FUNCTION 函数名 AS ‘具体类路径：包.类’;（4）.使用 $hive&gt;select 函数名(参数);自定义实现类如下(继承UDF)：]]></content>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase积累.md]]></title>
    <url>%2F2018%2F06%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fhbase%E7%A7%AF%E7%B4%AF%2F</url>
    <content type="text"><![CDATA[hbase积累 细节点1.Rowkey设计原则 1.1 长度原则 rowkey 在hbase以二进制码流,可以是任意字符串, 最大长度是64kb,实际应用主要是100~100bytes 长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性 1.2 散列原则:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是device_id+time. 1.3 RowKey唯一原则：必须在设计上保证其唯一性.hbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖. 2.Hbase的Regeion热点问题解决因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题 2.1 预分区预分区,”预”字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应预估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split. 2.1.2 salting(加盐)hbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用. 2.1.3 预习区具体方案 hbase预分区的相关操作,如shell形式,可直接在hbase shell操作.如 https://blog.csdn.net/xiao_jun_0820/article/details/24419793 java形式https://blog.csdn.net/qq_20641565/article/details/56482407 以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题. 2.1.4 hash分区 在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据. hbase优化确定优化目标沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。]]></content>
      <tags>
        <tag>日常总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark学习]]></title>
    <url>%2F2018%2F03%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[spark 学习 1spark 作为主流的实时计算引擎,需要高度掌握 spark介绍Apache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。 优点 易用 容错 spark体系整合 RDD详解RDD是什么RDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。 另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。 RDD的五个特性 有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 有一个函数计算每一个分片，这里指的是下面会提到的compute函数. 对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖. 可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。 可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”). 12345678910//只计算一次 protected def getPartitions: Array[Partition] //对一个分片进行计算，得出一个可遍历的结果 def compute(split: Partition, context: TaskContext): Iterator[T] //只计算一次，计算RDD对父RDD的依赖 protected def getDependencies: Seq[Dependency[_]] = deps //可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce @transient val partitioner: Option[Partitioner] = None //可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置 protected def getPreferredLocations(split: Partition): Seq[String] = Nil 为什么会产生RDDRDD数据集 并行集合 接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。 Hadoop数据集 Spark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。 此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。 Spark RDD算子 Transformation不触发提交作业，完成作业中间处理过程。 DStream什么是DStreamDiscretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图 计算则由spark engine来完成 spark java因为我是主要掌握的语言是java,从效率上来考虑,这里 参考博客https://blog.csdn.net/wangxiaotongfan/article/details/51395769 RDD详解https://blog.csdn.net/zuochang_liu/article/details/81459185 spark streaming学习https://blog.csdn.net/hellozhxy/article/details/81672845 spark java 使用指南https://blog.csdn.net/t1dmzks/article/details/70198430 sparkRDD算子介绍https://blog.csdn.net/wxycx11111/article/details/79123482 sparkRDD入门介绍https://github.com/zhaikaishun/spark_tutorial RDD算子介绍]]></content>
      <tags>
        <tag>学习spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark学习2]]></title>
    <url>%2F2018%2F03%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E5%AD%A6%E4%B9%A02%2F</url>
    <content type="text"><![CDATA[spark学习2spark 运行的四种模式 本地模式如1./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[1] ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100 standlone模式client./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100 cluster./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –deploy-mode cluster –supervise –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100 Yarn模式client模式123client模式：结果xshell可见：./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100 cluster模式./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster –executor-memory 1G –num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100 spark sql]]></content>
      <tags>
        <tag>学习spark2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark算子]]></title>
    <url>%2F2018%2F03%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fspark%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[spark 算子 123sparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.总得来讲spark的算子,本就是scala集合的一些高阶用法. Transformation(转换)不触发提交作业，完成作业中间处理过程。 parallelize (并行化)将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T] in scala 1sc.parallelize(List("shenzhen", "is a beautiful city")) in java 1JavaRDD&lt;String&gt; javaStringRDD = sc.parallelize(Arrays.asList("shenzhen", "is a beautiful city")); makeRDD只有scala版本的才有makeRDD ,与parallelize类似. textFile调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD in scala 1var lines = sc.textFile(inpath) 12// java JavaRDD&lt;String&gt; lines = sc.textFile(inpath); filter对RDD数据进行过滤 map接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应元素的值 map是一对一的关系 flatMap有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器 distinct去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 union两个RDD进行合并 intersectionRDD1.intersection(RDD2) 返回两个RDD的交集， 并且去重 intersection 需要混洗数据，比较浪费性能 subtractRDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 cartesiancartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大 mapToPair将元素该成key-value形式 flatMapToPair差异同mapToPair combineByKey该方法主要针对不同分区的同一key进行元素合并函数操作.需要对pairRDD进行 createCombiner 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建那个键对应的累加器的初始值 mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并 mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。reduceByKey接收一个函数，按照相同的key进行reduce操foldByKey该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 sortByKeySortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true groupByKeygroupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat cogroupgroupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 subtractByKey类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素join可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作RDD1.join(RDD2) fullOuterJoin全连接leftOuterJoinrightOuterJoin Actionfirst返回第一个元素 takerdd.take(n)返回第n个元素 collectrdd.collect() 返回 RDD 中的所有元素 countrdd.count() 返回 RDD 中的元素个数 countByValue各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} reduce并行整合RDD中所有数据 fold和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold toprdd.top(n)按照降序的或者指定的排序规则，返回前n个元素 takeOrderedrdd.take(n)对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 foreach对 RDD 中的每个元素使用给定的函数 countByKey以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} collectAsMap将pair类型(键值对类型)的RDD转换成map, 还是上面的例子 saveAsTextFilesaveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。 saveAsSequenceFilesaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。 saveAsObjectFilesaveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。 saveAsHadoopFilesaveAsNewAPIHadoopFilemapPartitionsmapPartitionsWithIndexHashPartitionerRangePartitioner自定义分区]]></content>
      <tags>
        <tag>spark学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop记录]]></title>
    <url>%2F2018%2F03%2F04%2F%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93%2Fold%2Fsqoop%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[将Mysql数据导入Hive中 命令:12345678sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://211.159.172.76:3306/solo--username root --password 125323Wkq --table tablename --hive-import --hive-table tablename 整库导入1234sqoop import-all-tables --connect jdbc:mysql://211.159.172.76:3306/ --username root --password 125323Wkq --hive-database solo -m 10 --create-hive-table --fields-terminated-by &quot;\t&quot;--hive-import --hive-database qianyang --hive-overwrite sqoop import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –hive-database blog –create-hive-table –hive-import –hive-overwrite -m 10 单表导入sqoop import –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –table b3_solo_article –target-dir /blog/article –hive-import –hive-database blog–fields-terminated-by “\t” –hive-table article –hive-overwrite–m 10 sqoop import –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –table b3_solo_article –target-dir /blog/article –hive-import –hive-database blog –create-hive-table –hive-table article –hive-overwrite -m 1]]></content>
      <tags>
        <tag>日常总结</tag>
      </tags>
  </entry>
</search>
